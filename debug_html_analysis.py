#!/usr/bin/env python3
import time
import os
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from src.analyze_titles import JobPostingSelectorAnalyzer

def create_debug_html_for_companies():
    """ì‹¤íŒ¨í•œ íšŒì‚¬ë“¤ì˜ ì‹¤ì œ HTMLì„ ìƒì„±í•˜ê³  ë¶„ì„"""

    # ì‹¤íŒ¨í•œ íšŒì‚¬ë“¤ì˜ URL ëª©ë¡
    failed_companies = {
        'ì‚¼ì„±ì „ì(ì£¼)': 'https://www.samsungcareers.com/hr/',
        'í˜„ëŒ€ìë™ì°¨(ì£¼)': 'https://recruit.hd.com/kr/mainLayout/apply',
        'LGì „ì(ì£¼)': 'https://careers.lg.com/apply',
        'SKì´ë…¸ë² ì´ì…˜(ì£¼)': 'https://recruit.skinnovation.com/ui/apply/applyList.do',
        'ë†í˜‘ê¸ˆìœµì§€ì£¼ ì£¼ì‹íšŒì‚¬': 'https://with.nonghyup.com/jbnf/jbnfLst.do'
    }

    # Chrome ì˜µì…˜ ì„¤ì •
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--disable-web-security')
    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    driver.set_page_load_timeout(30)

    analyzer = JobPostingSelectorAnalyzer()

    # ë””ë²„ê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    debug_dir = '/opt/airflow/debug_html'
    os.makedirs(debug_dir, exist_ok=True)

    print("=== ì‹¤íŒ¨í•œ íšŒì‚¬ë“¤ì˜ HTML ë¶„ì„ ë° ê°œì„  ë°©ì•ˆ ë„ì¶œ ===\\n")

    for company_name, url in failed_companies.items():
        print(f"ğŸ” {company_name} ë¶„ì„ ì¤‘...")

        try:
            # í˜ì´ì§€ ë¡œë“œ
            print(f"  - URL ì ‘ì†: {url}")
            driver.get(url)

            # JavaScript ì‹¤í–‰ ëŒ€ê¸°
            time.sleep(5)

            # ì¶”ê°€ ìŠ¤í¬ë¡¤ ë° ëŒ€ê¸° (AJAX ë¡œë”© ì™„ë£Œ ìœ„í•´)
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)
            driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(2)

            # HTML ê°€ì ¸ì˜¤ê¸°
            html_content = driver.page_source
            soup = BeautifulSoup(html_content, 'html.parser')

            # HTML íŒŒì¼ ì €ì¥
            html_file = os.path.join(debug_dir, f"{company_name}.html")
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            print(f"  - HTML ì €ì¥: {html_file}")

            # í˜„ì¬ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë¶„ì„
            print("  - í˜„ì¬ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë¶„ì„ ì‹œë„...")
            selector, titles = analyzer.find_best_selector(soup)

            if selector and titles:
                print(f"  âœ… ì„±ê³µ: {selector}")
                print(f"     ì˜ˆì‹œ: {titles[:3]}")
            else:
                print("  âŒ ì‹¤íŒ¨: ì„ íƒìë¥¼ ì°¾ì§€ ëª»í•¨")

                # ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
                print("  ğŸ” ì‹¤íŒ¨ ì›ì¸ ë¶„ì„:")

                # 1. ëª¨ë“  ë§í¬ í™•ì¸
                all_links = soup.find_all('a', href=True)
                print(f"    - ì „ì²´ ë§í¬ ìˆ˜: {len(all_links)}")

                # 2. ì±„ìš©ê³µê³  ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë§í¬ë“¤ ì°¾ê¸°
                job_related_links = []
                for link in all_links:
                    text = link.get_text(strip=True)
                    if text and ('ì±„ìš©' in text or 'ì¸í„´' in text or 'ê²½ë ¥' in text or 'ì‹ ì…' in text or
                               'job' in text.lower() or 'career' in text.lower() or 'recruit' in text.lower()):
                        job_related_links.append(text)

                print(f"    - ì±„ìš© ê´€ë ¨ ë§í¬ ìˆ˜: {len(job_related_links)}")
                if job_related_links:
                    print(f"    - ì˜ˆì‹œ: {job_related_links[:5]}")

                # 3. ê°€ëŠ¥í•œ ì±„ìš©ê³µê³  ì»¨í…Œì´ë„ˆ ì°¾ê¸°
                possible_containers = []
                for element in soup.find_all(['div', 'section', 'ul', 'table']):
                    element_id = element.get('id', '').lower()
                    element_classes = ' '.join(element.get('class', [])).lower()

                    if any(keyword in element_id + element_classes for keyword in
                          ['job', 'recruit', 'career', 'position', 'opening', 'notice', 'list']):
                        links_in_container = element.find_all('a', href=True)
                        if len(links_in_container) > 0:
                            possible_containers.append({
                                'tag': element.name,
                                'id': element_id,
                                'classes': element_classes,
                                'link_count': len(links_in_container),
                                'sample_texts': [link.get_text(strip=True)[:50] for link in links_in_container[:3]]
                            })

                print(f"    - ê°€ëŠ¥í•œ ì±„ìš©ê³µê³  ì»¨í…Œì´ë„ˆ ìˆ˜: {len(possible_containers)}")
                for container in possible_containers[:3]:
                    print(f"      * {container['tag']} (id: {container['id']}, classes: {container['classes']})")
                    print(f"        ë§í¬ ìˆ˜: {container['link_count']}, ìƒ˜í”Œ: {container['sample_texts']}")

            print()

        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print()

        time.sleep(2)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

    driver.quit()

    print("=== ë¶„ì„ ì™„ë£Œ ===")
    print(f"HTML íŒŒì¼ë“¤ì´ {debug_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ê°œì„  ë°©ì•ˆ ì œì‹œ
    print("\\n=== ê°œì„  ë°©ì•ˆ ===")
    print("1. JavaScript ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì¦ê°€")
    print("2. ë” ìœ ì—°í•œ í…ìŠ¤íŠ¸ í•„í„°ë§ ì¡°ê±´")
    print("3. ë™ì  ì»¨í…Œì´ë„ˆ íƒì§€ ì•Œê³ ë¦¬ì¦˜ ê°œì„ ")
    print("4. ë‹¤ì–‘í•œ ì„ íƒì íŒ¨í„´ ì¶”ê°€")

if __name__ == "__main__":
    create_debug_html_for_companies()