import os
import time
import pandas as pd
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime
import logging
from typing import Dict, List, Set
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

class JobMonitor:
    def __init__(self, base_dir: str):
        self.base_dir = base_dir
        self.data_dir = os.path.join(base_dir, 'data')
        self.config_path = os.path.join(self.data_dir, 'ì±„ìš©ê³µê³ _ëª©ë¡.csv')
        self.results_path = os.path.join(self.data_dir, 'job_postings_latest.csv')
        
        # .envì—ì„œ ì›¹í›… URL ë¡œë“œ
        self.webhook_url = os.getenv('SLACK_WEBHOOK_URL')
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

    def create_minimal_driver(self):
        """ìµœì†Œí•œì˜ ì„¤ì •ìœ¼ë¡œ ë¹ ë¥¸ ë“œë¼ì´ë²„"""
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-images")
        chrome_options.add_argument("--disable-css")
        
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        driver.set_page_load_timeout(10)
        return driver

    def get_html_content(self, url, use_selenium, driver=None):
        """URLì—ì„œ HTML ë‚´ìš©ì„ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if not use_selenium:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                response = requests.get(url, headers=headers, timeout=10)
                response.raise_for_status()
                return response.text
            else:
                if driver is None:
                    raise Exception("Seleniumì´ í•„ìš”í•˜ì§€ë§Œ ë“œë¼ì´ë²„ê°€ ì—†ìŠµë‹ˆë‹¤.")
                driver.get(url)
                time.sleep(2)
                return driver.page_source
        except Exception as e:
            self.logger.error(f"HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None

    def extract_current_jobs(self) -> Dict[str, Set[str]]:
        """í˜„ì¬ ì±„ìš©ê³µê³ ë¥¼ ì¶”ì¶œí•˜ì—¬ íšŒì‚¬ë³„ë¡œ ì •ë¦¬"""
        if not os.path.exists(self.config_path):
            self.logger.error(f"ì„¤ì • íŒŒì¼ '{self.config_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        try:
            df_config = pd.read_csv(self.config_path)
        except Exception as e:
            self.logger.error(f"ì„¤ì • íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {}

        # ì„ íƒìê°€ ì„¤ì •ëœ íšŒì‚¬ë§Œ ì²˜ë¦¬
        df_config = df_config[df_config['selector'].notna() & (df_config['selector'].str.strip() != '')]
        
        if df_config.empty:
            self.logger.warning("ì„ íƒìê°€ ì„¤ì •ëœ íšŒì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        selenium_needed = df_config['selenium_required'].any()
        driver = None
        
        if selenium_needed:
            self.logger.info("Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")
            driver = self.create_minimal_driver()

        current_jobs = {}
        
        self.logger.info("ì±„ìš© ê³µê³  ì¶”ì¶œ ì‹œì‘...")
        start_time = time.time()
        
        try:
            for index, row in df_config.iterrows():
                company_name = row['íšŒì‚¬ëª…']
                url = row['ì±„ìš©ê³µê³  URL']
                use_selenium = row['selenium_required']
                selector = row['selector']
                
                self.logger.info(f"[{index+1}/{len(df_config)}] {company_name} ì²˜ë¦¬ ì¤‘...")
                
                html_content = self.get_html_content(url, use_selenium, driver)
                if not html_content:
                    continue
                    
                try:
                    soup = BeautifulSoup(html_content, 'html.parser')
                    postings = soup.select(selector)
                    
                    if not postings:
                        self.logger.warning(f"    '{selector}' ì„ íƒìë¡œ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í•¨")
                        continue

                    job_titles = set()
                    for post in postings:
                        title = post.get_text(strip=True)
                        if len(title) > 5:
                            job_titles.add(title)
                    
                    if job_titles:
                        current_jobs[company_name] = job_titles
                        self.logger.info(f"    ì„±ê³µ: {len(job_titles)}ê°œ ê³µê³  ì¶”ì¶œë¨")
                    
                except Exception as e:
                    self.logger.error(f"    HTML íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue
                    
        finally:
            if driver:
                driver.quit()
        
        elapsed_time = time.time() - start_time
        total_jobs = sum(len(jobs) for jobs in current_jobs.values())
        self.logger.info(f"ì´ {total_jobs}ê°œ ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ (ì‹¤í–‰ì‹œê°„: {elapsed_time:.1f}ì´ˆ)")
        
        return current_jobs

    def load_existing_jobs(self) -> Dict[str, Set[str]]:
        """ê¸°ì¡´ì— ì €ì¥ëœ ì±„ìš©ê³µê³  ë¡œë“œ"""
        if not os.path.exists(self.results_path):
            return {}
        
        try:
            df = pd.read_csv(self.results_path)
            existing_jobs = {}
            
            for company in df['company_name'].unique():
                company_jobs = set(
                    df[df['company_name'] == company]['job_posting_title'].tolist()
                )
                existing_jobs[company] = company_jobs
            
            return existing_jobs
        except Exception as e:
            self.logger.error(f"ê¸°ì¡´ ê³µê³  ë¡œë“œ ì˜¤ë¥˜: {e}")
            return {}

    def find_new_jobs(self, current_jobs: Dict[str, Set[str]], existing_jobs: Dict[str, Set[str]]) -> Dict[str, List[str]]:
        """ìƒˆë¡œìš´ ì±„ìš©ê³µê³  ì°¾ê¸°"""
        new_jobs = {}
        
        for company, current_titles in current_jobs.items():
            existing_titles = existing_jobs.get(company, set())
            new_titles = current_titles - existing_titles
            
            if new_titles:
                new_jobs[company] = list(new_titles)
                self.logger.info(f"{company}: {len(new_titles)}ê°œ ìƒˆë¡œìš´ ê³µê³  ë°œê²¬")
        
        return new_jobs

    def save_jobs(self, current_jobs: Dict[str, Set[str]]):
        """í˜„ì¬ ì±„ìš©ê³µê³ ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not current_jobs:
            self.logger.warning("ì €ì¥í•  ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        all_postings = []
        crawl_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        for company_name, job_titles in current_jobs.items():
            for title in job_titles:
                all_postings.append({
                    "company_name": company_name,
                    "job_posting_title": title,
                    "crawl_datetime": crawl_datetime
                })
        
        try:
            df = pd.DataFrame(all_postings)
            df.to_csv(self.results_path, index=False, encoding='utf-8-sig')
            self.logger.info(f"ê²°ê³¼ë¥¼ '{self.results_path}'ì— ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")

    def send_slack_notification(self, new_jobs: Dict[str, List[str]]):
        """ìŠ¬ë™ìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡"""
        if not new_jobs:
            self.logger.info("ì „ì†¡í•  ìƒˆë¡œìš´ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        if not self.webhook_url:
            self.logger.error("âŒ ìŠ¬ë™ ì›¹í›… URLì´ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.logger.error("   .env íŒŒì¼ì— SLACK_WEBHOOK_URL=your_webhook_url ì„ ì¶”ê°€í•˜ì„¸ìš”.")
            return

        # ë©”ì‹œì§€ í¬ë§·íŒ…
        current_time = datetime.now().strftime('%H:%M')
        message_parts = [f"ğŸš€ *ìƒˆë¡œìš´ ì±„ìš©ê³µê³  ì•Œë¦¼* ({current_time})\n"]
        
        total_new_jobs = sum(len(jobs) for jobs in new_jobs.values())
        message_parts.append(f"ğŸ“Š ì´ {total_new_jobs}ê°œì˜ ìƒˆë¡œìš´ ê³µê³ ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!\n")
        
        for company_name, jobs in new_jobs.items():
            message_parts.append(f"\nğŸ“¢ *{company_name}*")
            for job_title in jobs:
                # ìŠ¬ë™ íŠ¹ìˆ˜ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
                escaped_title = job_title.replace("<", "&lt;").replace(">", "&gt;").replace("&", "&amp;")
                message_parts.append(f"â€¢ {escaped_title}")
        
        message = "\n".join(message_parts)
        
        payload = {
            "text": message,
            "username": "ì±„ìš©ê³µê³  ë´‡",
            "icon_emoji": ":briefcase:"
        }
        
        try:
            response = requests.post(self.webhook_url, json=payload, timeout=10)
            if response.status_code == 200:
                self.logger.info("âœ… ìŠ¬ë™ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
            else:
                self.logger.error(f"âŒ ìŠ¬ë™ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")
        except Exception as e:
            self.logger.error(f"âŒ ìŠ¬ë™ ì•Œë¦¼ ì „ì†¡ ì˜¤ë¥˜: {e}")

    def run_monitoring(self):
        """ì „ì²´ ëª¨ë‹ˆí„°ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        self.logger.info("=" * 60)
        self.logger.info("ì±„ìš©ê³µê³  ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        self.logger.info("=" * 60)
        
        # 1ë‹¨ê³„: ê¸°ì¡´ ê³µê³  ë¡œë“œ
        existing_jobs = self.load_existing_jobs()
        if existing_jobs:
            total_existing = sum(len(jobs) for jobs in existing_jobs.values())
            self.logger.info(f"ê¸°ì¡´ ê³µê³ : {total_existing}ê°œ")
        else:
            self.logger.info("ê¸°ì¡´ ê³µê³  ì—†ìŒ (ì²« ì‹¤í–‰)")
        
        # 2ë‹¨ê³„: í˜„ì¬ ê³µê³  ì¶”ì¶œ
        current_jobs = self.extract_current_jobs()
        if not current_jobs:
            self.logger.error("ì±„ìš©ê³µê³  ì¶”ì¶œ ì‹¤íŒ¨")
            return False
        
        # 3ë‹¨ê³„: ìƒˆë¡œìš´ ê³µê³  ì°¾ê¸°
        new_jobs = self.find_new_jobs(current_jobs, existing_jobs)
        
        # 4ë‹¨ê³„: ìƒˆë¡œìš´ ê³µê³ ê°€ ìˆìœ¼ë©´ ì•Œë¦¼ ì „ì†¡
        if new_jobs:
            total_new = sum(len(jobs) for jobs in new_jobs.values())
            self.logger.info(f"ğŸ‰ ì´ {total_new}ê°œì˜ ìƒˆë¡œìš´ ê³µê³  ë°œê²¬!")
            self.send_slack_notification(new_jobs)
        else:
            self.logger.info("ìƒˆë¡œìš´ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # 5ë‹¨ê³„: í˜„ì¬ ê³µê³ ë¡œ ì—…ë°ì´íŠ¸
        self.save_jobs(current_jobs)
        
        self.logger.info("=" * 60)
        self.logger.info("ì±„ìš©ê³µê³  ëª¨ë‹ˆí„°ë§ ì™„ë£Œ")
        self.logger.info("=" * 60)
        
        return True

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    monitor = JobMonitor(base_dir)
    monitor.run_monitoring()

if __name__ == "__main__":
    main()