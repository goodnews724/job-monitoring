import os
import time
import pandas as pd
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime
import logging
from typing import Dict, List, Set, Tuple
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

class JobMonitor:
    def __init__(self, base_dir: str):
        self.base_dir = base_dir
        self.data_dir = os.path.join(base_dir, 'data')
        self.config_path = os.path.join(self.data_dir, 'ì±„ìš©ê³µê³ _ëª©ë¡.csv')
        self.results_path = os.path.join(self.data_dir, 'job_postings_latest.csv')
        
        # .envì—ì„œ ì›¹í›… URL ë¡œë“œ
        self.webhook_url = os.getenv('SLACK_WEBHOOK_URL')
        
        # íšŒì‚¬ë³„ URL ì €ì¥ (ë§í¬ìš©)
        self.company_urls = {}
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

    def create_minimal_driver(self):
        """ìµœì†Œí•œì˜ ì„¤ì •ìœ¼ë¡œ ë¹ ë¥¸ ë“œë¼ì´ë²„"""
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-images")
        chrome_options.add_argument("--disable-css")
        
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        driver.set_page_load_timeout(10)
        return driver

    def get_html_content(self, url, use_selenium, driver=None):
        """URLì—ì„œ HTML ë‚´ìš©ì„ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if not use_selenium:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                response = requests.get(url, headers=headers, timeout=10)
                response.raise_for_status()
                return response.text
            else:
                if driver is None:
                    raise Exception("Seleniumì´ í•„ìš”í•˜ì§€ë§Œ ë“œë¼ì´ë²„ê°€ ì—†ìŠµë‹ˆë‹¤.")
                driver.get(url)
                time.sleep(2)
                return driver.page_source
        except Exception as e:
            self.logger.error(f"HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None

    def extract_current_jobs(self) -> Tuple[Dict[str, Set[str]], List[Dict]]:
        """í˜„ì¬ ì±„ìš©ê³µê³ ë¥¼ ì¶”ì¶œí•˜ì—¬ íšŒì‚¬ë³„ë¡œ ì •ë¦¬, ì‹¤íŒ¨í•œ íšŒì‚¬ ëª©ë¡ë„ ë°˜í™˜"""
        if not os.path.exists(self.config_path):
            self.logger.error(f"ì„¤ì • íŒŒì¼ '{self.config_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}, []

        try:
            df_config = pd.read_csv(self.config_path)
        except Exception as e:
            self.logger.error(f"ì„¤ì • íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {}, []

        # ì„ íƒìê°€ ì„¤ì •ëœ íšŒì‚¬ë§Œ ì²˜ë¦¬
        df_config = df_config[df_config['selector'].notna() & (df_config['selector'].str.strip() != '')]
        
        if df_config.empty:
            self.logger.warning("ì„ íƒìê°€ ì„¤ì •ëœ íšŒì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}, []

        selenium_needed = df_config['selenium_required'].any()
        driver = None
        
        if selenium_needed:
            self.logger.info("Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")
            driver = self.create_minimal_driver()

        current_jobs = {}
        failed_companies = []
        self.company_urls = {}  # URL ì €ì¥ ì´ˆê¸°í™”
        
        self.logger.info("ì±„ìš© ê³µê³  ì¶”ì¶œ ì‹œì‘...")
        start_time = time.time()
        
        try:
            for index, row in df_config.iterrows():
                company_name = row['íšŒì‚¬ëª…']
                url = row['ì±„ìš©ê³µê³  URL']
                use_selenium = row['selenium_required']
                selector = row['selector']
                
                # íšŒì‚¬ë³„ URL ì €ì¥ (ìŠ¬ë™ ë§í¬ìš©)
                self.company_urls[company_name] = url
                
                self.logger.info(f"[{index+1}/{len(df_config)}] {company_name} ì²˜ë¦¬ ì¤‘...")
                
                html_content = self.get_html_content(url, use_selenium, driver)
                if not html_content:
                    failed_companies.append({
                        'company': company_name,
                        'reason': 'HTML ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨',
                        'url': url
                    })
                    continue
                    
                try:
                    soup = BeautifulSoup(html_content, 'html.parser')
                    postings = soup.select(selector)
                    
                    if not postings:
                        self.logger.warning(f"    '{selector}' ì„ íƒìë¡œ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í•¨")
                        failed_companies.append({
                            'company': company_name,
                            'reason': f"ì„ íƒì '{selector}'ë¡œ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í•¨",
                            'url': url
                        })
                        continue

                    job_titles = set()
                    for post in postings:
                        title = post.get_text(strip=True)
                        if len(title) > 5:
                            job_titles.add(title)
                    
                    if job_titles:
                        current_jobs[company_name] = job_titles
                        self.logger.info(f"    ì„±ê³µ: {len(job_titles)}ê°œ ê³µê³  ì¶”ì¶œë¨")
                    else:
                        failed_companies.append({
                            'company': company_name,
                            'reason': 'ìœ íš¨í•œ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í•¨',
                            'url': url
                        })
                    
                except Exception as e:
                    self.logger.error(f"    HTML íŒŒì‹± ì‹¤íŒ¨: {e}")
                    failed_companies.append({
                        'company': company_name,
                        'reason': f'HTML íŒŒì‹± ì‹¤íŒ¨: {str(e)}',
                        'url': url
                    })
                    continue
                    
        finally:
            if driver:
                driver.quit()
        
        elapsed_time = time.time() - start_time
        total_jobs = sum(len(jobs) for jobs in current_jobs.values())
        self.logger.info(f"ì´ {total_jobs}ê°œ ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ (ì‹¤í–‰ì‹œê°„: {elapsed_time:.1f}ì´ˆ)")
        
        if failed_companies:
            self.logger.warning(f"âš ï¸  {len(failed_companies)}ê°œ íšŒì‚¬ì—ì„œ í¬ë¡¤ë§ ì‹¤íŒ¨")
        
        return current_jobs, failed_companies

    def load_existing_jobs(self) -> Dict[str, Set[str]]:
        """ê¸°ì¡´ì— ì €ì¥ëœ ì±„ìš©ê³µê³  ë¡œë“œ"""
        if not os.path.exists(self.results_path):
            return {}
        
        try:
            df = pd.read_csv(self.results_path)
            existing_jobs = {}
            
            for company in df['company_name'].unique():
                company_jobs = set(
                    df[df['company_name'] == company]['job_posting_title'].tolist()
                )
                existing_jobs[company] = company_jobs
            
            return existing_jobs
        except Exception as e:
            self.logger.error(f"ê¸°ì¡´ ê³µê³  ë¡œë“œ ì˜¤ë¥˜: {e}")
            return {}

    def check_suspicious_results(self, current_jobs: Dict[str, Set[str]], 
                               existing_jobs: Dict[str, Set[str]], 
                               new_jobs: Dict[str, List[str]]) -> List[str]:
        """ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê²°ê³¼ ì²´í¬í•˜ì—¬ ê²½ê³  ë©”ì‹œì§€ ìƒì„±"""
        warnings = []
        
        # 1. ëª¨ë“  ê³µê³ ê°€ ìƒˆë¡œìš´ ê³µê³ ì¸ ê²½ìš° ì²´í¬
        for company, new_job_list in new_jobs.items():
            current_count = len(current_jobs.get(company, set()))
            existing_count = len(existing_jobs.get(company, set()))
            new_count = len(new_job_list)
            
            # ê¸°ì¡´ ê³µê³ ê°€ ìˆì—ˆëŠ”ë° í˜„ì¬ ê³µê³ ì˜ 90% ì´ìƒì´ ìƒˆë¡œìš´ ê³µê³ ì¸ ê²½ìš°
            if existing_count > 0 and current_count > 0:
                new_ratio = new_count / current_count
                if new_ratio >= 0.9:  # 90% ì´ìƒì´ ìƒˆë¡œìš´ ê³µê³ 
                    warnings.append(
                        f"{company}: ê³µê³ ì˜ {new_ratio:.0%}ê°€ ìƒˆë¡œìš´ ê³µê³ ì…ë‹ˆë‹¤"
                    )
        
        # 2. ê¸°ì¡´ì— ìˆë˜ íšŒì‚¬ê°€ ê°‘ìê¸° ê³µê³ ê°€ ì—†ì–´ì§„ ê²½ìš°
        missing_companies = set(existing_jobs.keys()) - set(current_jobs.keys())
        if missing_companies:
            warnings.append(
                f"ë‹¤ìŒ íšŒì‚¬ë“¤ì˜ ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_companies)}"
            )
        
        return warnings

    def find_new_jobs(self, current_jobs: Dict[str, Set[str]], existing_jobs: Dict[str, Set[str]]) -> Dict[str, List[str]]:
        """ìƒˆë¡œìš´ ì±„ìš©ê³µê³  ì°¾ê¸°"""
        new_jobs = {}
        
        for company, current_titles in current_jobs.items():
            existing_titles = existing_jobs.get(company, set())
            new_titles = current_titles - existing_titles
            
            if new_titles:
                new_jobs[company] = list(new_titles)
                self.logger.info(f"{company}: {len(new_titles)}ê°œ ìƒˆë¡œìš´ ê³µê³  ë°œê²¬")
        
        return new_jobs

    def save_jobs(self, current_jobs: Dict[str, Set[str]]):
        """í˜„ì¬ ì±„ìš©ê³µê³ ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not current_jobs:
            self.logger.warning("ì €ì¥í•  ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        all_postings = []
        crawl_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        for company_name, job_titles in current_jobs.items():
            for title in job_titles:
                all_postings.append({
                    "company_name": company_name,
                    "job_posting_title": title,
                    "crawl_datetime": crawl_datetime
                })
        
        try:
            df = pd.DataFrame(all_postings)
            df.to_csv(self.results_path, index=False, encoding='utf-8-sig')
            self.logger.info(f"ê²°ê³¼ë¥¼ '{self.results_path}'ì— ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")

    def send_slack_notification(self, new_jobs: Dict[str, List[str]], 
                            warnings: List[str] = None, 
                            failed_companies: List[Dict] = None):
        """ê°„ê²°í•œ ìŠ¬ë™ ì•Œë¦¼ ì „ì†¡ - ì‚¬ìš©ì ì¹œí™”ì """
        
        if not self.webhook_url:
            self.logger.error("âŒ ìŠ¬ë™ ì›¹í›… URLì´ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.logger.error("   .env íŒŒì¼ì— SLACK_WEBHOOK_URL=your_webhook_url ì„ ì¶”ê°€í•˜ì„¸ìš”.")
            return

        # ì•Œë¦¼ì´ í•„ìš”í•œ ê²½ìš°ë§Œ ë©”ì‹œì§€ ì „ì†¡
        has_new_jobs = bool(new_jobs)
        has_warnings = bool(warnings)
        has_failures = bool(failed_companies)
        
        # ì•„ë¬´ ì´ìŠˆë„ ì—†ìœ¼ë©´ ë©”ì‹œì§€ ì „ì†¡í•˜ì§€ ì•ŠìŒ
        if not has_new_jobs and not has_warnings and not has_failures:
            self.logger.info("ìƒˆë¡œìš´ ê³µê³ ë‚˜ ë¬¸ì œì‚¬í•­ì´ ì—†ì–´ ìŠ¬ë™ ì•Œë¦¼ì„ ë³´ë‚´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        current_time = datetime.now().strftime('%H:%M')
        message_parts = []
        
        # ğŸ‰ ìƒˆë¡œìš´ ê³µê³ ê°€ ìˆëŠ” ê²½ìš°
        if has_new_jobs:
            total_new_jobs = sum(len(jobs) for jobs in new_jobs.values())
            message_parts.append(f"ğŸ‰ *ìƒˆë¡œìš´ ì±„ìš©ê³µê³  {total_new_jobs}ê°œ ë°œê²¬!* ({current_time})")
            message_parts.append("")
            
            for company_name, jobs in new_jobs.items():
                company_url = self.company_urls.get(company_name, "")
                if company_url:
                    linked_company_name = f"<{company_url}|{company_name}>"
                else:
                    linked_company_name = company_name
                
                message_parts.append(f"ğŸ“¢ *{linked_company_name}* - {len(jobs)}ê°œ")
                
                # ëª¨ë“  ê³µê³  í‘œì‹œ
                for job_title in jobs:
                    escaped_title = job_title.replace("<", "&lt;").replace(">", "&gt;").replace("&", "&amp;")
                    message_parts.append(f"â€¢ {escaped_title}")
                
                message_parts.append("")  # íšŒì‚¬ë³„ êµ¬ë¶„ì„ ìœ„í•œ ë¹ˆ ì¤„
        
        # âš ï¸ ë¬¸ì œê°€ ìˆëŠ” ê²½ìš°ë§Œ ê°„ë‹¨íˆ ì•Œë¦¼
        if has_warnings or has_failures:
            if not has_new_jobs:
                message_parts.append(f"âš ï¸ *ì±„ìš©ê³µê³  í™•ì¸ í•„ìš”* ({current_time})")
                message_parts.append("")
            
            # ê°„ë‹¨í•œ ê²½ê³  ë©”ì‹œì§€
            if has_warnings:
                suspicious_companies = []
                for warning in warnings:
                    if ":" in warning:
                        company_part = warning.split(":")[0]
                        suspicious_companies.append(company_part)
                
                if suspicious_companies:
                    message_parts.append("âš ï¸ ì¼ë¶€ íšŒì‚¬ì—ì„œ ë§ì€ ê³µê³ ê°€ ìƒˆë¡œ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    message_parts.append(f"í™•ì¸ í•„ìš”: {', '.join(suspicious_companies)}")
                    message_parts.append("")
            
            # ì‹¤íŒ¨í•œ íšŒì‚¬ë“¤ ê°„ë‹¨íˆ í‘œì‹œ
            if has_failures:
                failed_company_names = [fail_info['company'] for fail_info in failed_companies]
                message_parts.append(f"âŒ {len(failed_company_names)}ê°œ íšŒì‚¬ì—ì„œ ê³µê³ ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:")
                message_parts.append(f"{', '.join(failed_company_names)}")
                message_parts.append("")
                message_parts.append("ğŸ’¡ í•´ë‹¹ íšŒì‚¬ ì‚¬ì´íŠ¸ë¥¼ ì§ì ‘ í™•ì¸í•´ë³´ì„¸ìš”.")
        
        message = "\n".join(message_parts).strip()
        
        # ì•„ì´ì½˜ ì„ íƒ
        if has_warnings or has_failures:
            icon = ":warning:"
        else:
            icon = ":tada:"
        
        payload = {
            "text": message,
            "username": "ì±„ìš©ê³µê³  ì•Œë¦¼",
            "icon_emoji": icon
        }
        
        try:
            response = requests.post(self.webhook_url, json=payload, timeout=10)
            if response.status_code == 200:
                self.logger.info("âœ… ìŠ¬ë™ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
            else:
                self.logger.error(f"âŒ ìŠ¬ë™ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")
        except Exception as e:
            self.logger.error(f"âŒ ìŠ¬ë™ ì•Œë¦¼ ì „ì†¡ ì˜¤ë¥˜: {e}")

    def run_monitoring(self):
        """ì „ì²´ ëª¨ë‹ˆí„°ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        self.logger.info("=" * 60)
        self.logger.info("ì±„ìš©ê³µê³  ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        self.logger.info("=" * 60)
        
        # 1ë‹¨ê³„: ê¸°ì¡´ ê³µê³  ë¡œë“œ
        existing_jobs = self.load_existing_jobs()
        if existing_jobs:
            total_existing = sum(len(jobs) for jobs in existing_jobs.values())
            self.logger.info(f"ê¸°ì¡´ ê³µê³ : {total_existing}ê°œ")
        else:
            self.logger.info("ê¸°ì¡´ ê³µê³  ì—†ìŒ (ì²« ì‹¤í–‰)")
        
        # 2ë‹¨ê³„: í˜„ì¬ ê³µê³  ì¶”ì¶œ
        current_jobs, failed_companies = self.extract_current_jobs()
        
        # 3ë‹¨ê³„: ìƒˆë¡œìš´ ê³µê³  ì°¾ê¸°
        new_jobs = self.find_new_jobs(current_jobs, existing_jobs)
        
        # 4ë‹¨ê³„: ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê²°ê³¼ ì²´í¬
        warnings = self.check_suspicious_results(current_jobs, existing_jobs, new_jobs)
        
        # 5ë‹¨ê³„: í•„ìš”í•œ ê²½ìš°ë§Œ ìŠ¬ë™ ì•Œë¦¼ ì „ì†¡
        if new_jobs:
            total_new = sum(len(jobs) for jobs in new_jobs.values())
            self.logger.info(f"ğŸ‰ ì´ {total_new}ê°œì˜ ìƒˆë¡œìš´ ê³µê³  ë°œê²¬!")
        
        if warnings:
            self.logger.warning("âš ï¸ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê²°ê³¼ ê°ì§€")
        
        if failed_companies:
            self.logger.error(f"âŒ {len(failed_companies)}ê°œ íšŒì‚¬ì—ì„œ í¬ë¡¤ë§ ì‹¤íŒ¨")
        
        # ì•Œë¦¼ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì „ì†¡
        self.send_slack_notification(new_jobs, warnings, failed_companies)
        
        if not new_jobs and not warnings and not failed_companies:
            self.logger.info("ìƒˆë¡œìš´ ê³µê³  ë° ì´ìƒì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤. (ìŠ¬ë™ ì•Œë¦¼ ì—†ìŒ)")
        
        # 6ë‹¨ê³„: í˜„ì¬ ê³µê³ ë¡œ ì—…ë°ì´íŠ¸
        if current_jobs:
            self.save_jobs(current_jobs)
        
        self.logger.info("=" * 60)
        self.logger.info("ì±„ìš©ê³µê³  ëª¨ë‹ˆí„°ë§ ì™„ë£Œ")
        self.logger.info("=" * 60)
        
        return True

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    monitor = JobMonitor(base_dir)
    monitor.run_monitoring()

if __name__ == "__main__":
    main()