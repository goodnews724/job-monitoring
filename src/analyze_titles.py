import os
import pandas as pd
from bs4 import BeautifulSoup
from collections import Counter
import re
from typing import List, Tuple, Optional, Dict, Set
import logging

class JobPostingSelectorAnalyzer:
    """ì±„ìš©ê³µê³  ì„ íƒì ë¶„ì„ê¸° (ê³µìœ  ìºì‹œ ê¸°ëŠ¥ ì¶”ê°€)"""
    
    def __init__(self):
        self.keywords = [
            'ê²½ë ¥', 'ì‹ ì…', 'ì¸í„´', 'ì±„ìš©', 'ëª¨ì§‘', 'ê°œë°œ', 'ë””ìì¸', 'ê¸°íš',
            'ë§ˆì¼€íŒ…', 'ì—”ì§€ë‹ˆì–´', 'ë§¤ë‹ˆì €', 'ë¦¬ë“œ', 'ìš´ì˜', 'ë°ì´í„°', 'ë‹´ë‹¹ì',
            'engineer', 'designer', 'manager', 'developer', 'marketing', 'data', 'product', 'QA', 'PM'
        ]
        self.blacklist = ['nav', 'footer', 'header', 'menu', 'sitemap', 'aside', 'sidebar']
        self.logger = self._setup_logger()

    def _setup_logger(self) -> logging.Logger:
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        return logger

    def _validate_selector(self, soup: BeautifulSoup, selector: str) -> Optional[List[str]]:
        """ê¸°ì¡´ ì„ íƒìì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤."""
        try:
            elements = soup.select(selector)
            if not elements:
                return None
            
            titles = [elem.get_text(strip=True) for elem in elements]
            valid_titles = [t for t in titles if 10 < len(t) < 150]
            
            if len(valid_titles) >= 3:
                return valid_titles
            else:
                return None
        except Exception:
            return None

    def find_best_selector(self, soup: BeautifulSoup) -> Tuple[Optional[str], List[str]]:
        potential_elements = []
        for element in soup.find_all(['p', 'div', 'span', 'a', 'strong', 'h2', 'h3', 'h4', 'li', 'dt', 'td']):
            if any(parent.name in self.blacklist or any(b in (parent.get('class', []) + ([parent.get('id')] if parent.get('id') else [])) for b in self.blacklist) for parent in element.find_parents()):
                continue
            
            text = element.get_text(strip=True)
            if any(re.search(keyword, text, re.IGNORECASE) for keyword in self.keywords) and 5 < len(text) < 150:
                potential_elements.append(element)

        if len(potential_elements) < 3:
            return None, []

        parent_counter = Counter()
        for element in potential_elements:
            for i, parent in enumerate(element.find_parents(limit=3)):
                parent_counter[parent] += 1
        
        if not parent_counter:
            return None, []

        container = parent_counter.most_common(1)[0][0]

        child_selector_counter = Counter()
        for element in potential_elements:
            if container in element.find_parents():
                direct_child = element
                while direct_child.find_parent() != container and direct_child.find_parent() is not None:
                    direct_child = direct_child.find_parent()
                
                if direct_child and direct_child.name:
                    tag = direct_child.name
                    classes = '.' + '.'.join(sorted(direct_child.get('class', []))) if direct_child.get('class') else ''
                    child_selector_counter[f"{tag}{classes}"] += 1

        if not child_selector_counter:
            return None, []

        best_child_selector = child_selector_counter.most_common(1)[0][0]
        
        container_tag = container.name
        container_classes = '.' + '.'.join(sorted(container.get('class', []))) if container.get('class') else ''
        
        final_selector_candidates = Counter()
        try:
            for element in soup.select(f"{container_tag}{container_classes} > {best_child_selector}"):
                for p_elem in element.find_all(['p', 'strong', 'h2', 'h3', 'h4', 'div']):
                     if p_elem.get('class'):
                         cls = '.' + '.'.join(p_elem.get('class'))
                         final_selector_candidates[f"{best_child_selector} {p_elem.name}{cls}"] += 1
        except Exception:
            pass

        if not final_selector_candidates:
             final_selector = best_child_selector
        else:
            best_selector_str = None
            max_score = -1
            for selector, count in final_selector_candidates.most_common():
                if count < 3: continue
                try:
                    texts = [elem.get_text(strip=True) for elem in soup.select(selector)]
                    if not texts: continue
                    
                    valid_texts = [t for t in texts if 10 < len(t) < 150]
                    if len(valid_texts) < 3: continue

                    avg_len = sum(len(t) for t in valid_texts) / len(valid_texts)
                    
                    keyword_score = sum(1 for t in valid_texts if any(re.search(k, t, re.IGNORECASE) for k in self.keywords))
                    keyword_ratio = keyword_score / len(valid_texts)
                    
                    score = avg_len * keyword_ratio

                    if score > max_score:
                        max_score = score
                        best_selector_str = selector
                except Exception:
                    continue
            
            if best_selector_str:
                final_selector = best_selector_str
            else:
                final_selector = final_selector_candidates.most_common(1)[0][0]

        try:
            titles = [elem.get_text(strip=True) for elem in soup.select(final_selector)]
            valid_titles = [t for t in titles if len(t) > 5]

            if len(valid_titles) >= 3:
                return final_selector, valid_titles
        except Exception:
            pass

        try:
            titles = [t.get_text(strip=True) for t in soup.select(best_child_selector)]
            valid_titles = [t for t in titles if len(t) > 5]
            if len(valid_titles) >= 3:
                return best_child_selector, valid_titles
        except Exception:
            return None, []

        return None, []

class ConfigManager:
    def __init__(self, config_path: str, html_dir: str):
        self.config_path = config_path
        self.html_dir = html_dir
        self.analyzer = JobPostingSelectorAnalyzer()
        self.logger = logging.getLogger(__name__)
        self.verified_selector_cache: Set[str] = set()
    
    def update_selectors(self) -> bool:
        try:
            df_config = pd.read_csv(self.config_path)
        except FileNotFoundError:
            self.logger.error(f"ì„¤ì • íŒŒì¼ '{self.config_path}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        # ì„ íƒìê°€ ì—†ëŠ” íšŒì‚¬ë§Œ í•„í„°ë§
        companies_without_selector = df_config[
            df_config['selector'].isna() | 
            (df_config['selector'].str.strip() == '') |
            df_config['selector'].isnull()
        ]
        
        if companies_without_selector.empty:
            print("=" * 60)
            print("ëª¨ë“  íšŒì‚¬ì— ì´ë¯¸ ì„ íƒìê°€ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            print("=" * 60)
            return True
        
        print("=" * 60)
        print(f"ì„ íƒìê°€ ì—†ëŠ” {len(companies_without_selector)}ê°œ íšŒì‚¬ ë¶„ì„ ì‹œì‘")
        print("=" * 60)
        
        analyzed_selectors = self._analyze_html_files(companies_without_selector)
        
        # ê²°ê³¼ë¥¼ ì›ë³¸ DataFrameì— ë°˜ì˜
        for company_name, selector in analyzed_selectors.items():
            df_config.loc[df_config['íšŒì‚¬ëª…'] == company_name, 'selector'] = selector

        self._save_config(df_config)
        self._print_summary(analyzed_selectors, len(companies_without_selector))
        return True

    def _analyze_html_files(self, companies_df: pd.DataFrame) -> Dict[str, str]:
        analyzed_selectors = {}
        
        for idx, (index, row) in enumerate(companies_df.iterrows(), 1):
            company_name = row['íšŒì‚¬ëª…']
            print(f"\n[{idx}/{len(companies_df)}] {company_name} ë¶„ì„ ì¤‘...")
            
            html_file_path = os.path.join(self.html_dir, f"{company_name}.html")
            if not os.path.exists(html_file_path):
                print(f"  âŒ HTML íŒŒì¼ì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            
            try:
                with open(html_file_path, 'r', encoding='utf-8') as f:
                    soup = BeautifulSoup(f, 'html.parser')
                
                # 1ë‹¨ê³„: ì´ë²ˆ ì‹¤í–‰ì—ì„œ ê²€ì¦ëœ ê³µìœ  ìºì‹œ í™•ì¸
                is_cached = False
                for cached_selector in self.verified_selector_cache:
                    found_titles = self.analyzer._validate_selector(soup, cached_selector)
                    if found_titles:
                        print(f"  âœ… [ê³µìœ  ìºì‹œ ì ì¤‘] ë‹¤ë¥¸ íšŒì‚¬ì—ì„œ ê²€ì¦ëœ ì„ íƒì '{cached_selector}'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        analyzed_selectors[company_name] = cached_selector
                        for title in found_titles[:3]:
                            print(f"    - {title}")
                        is_cached = True
                        break
                
                if is_cached:
                    continue

                # 2ë‹¨ê³„: ìƒˆë¡œìš´ ë¶„ì„ ì‹¤í–‰
                print("  ğŸ” ìƒˆë¡œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
                best_selector, found_titles = self.analyzer.find_best_selector(soup)
                
                if best_selector and found_titles:
                    print(f"  âœ… ì„±ê³µ! ìƒˆë¡œìš´ ì„ íƒì: '{best_selector}'")
                    analyzed_selectors[company_name] = best_selector
                    self.verified_selector_cache.add(best_selector)
                    for title in found_titles[:3]:
                        print(f"    - {title}")
                else:
                    print("  âŒ ë¶„ì„ ì‹¤íŒ¨: ìœ íš¨í•œ íŒ¨í„´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                self.logger.error(f"  âŒ {company_name} ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                
        return analyzed_selectors

    def _save_config(self, df_config: pd.DataFrame) -> bool:
        try:
            df_config.to_csv(self.config_path, index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ë¥¼ '{self.config_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            return True
        except Exception as e:
            self.logger.error(f"ì„¤ì • íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _print_summary(self, analyzed_selectors: Dict[str, str], total_companies: int):
        print("\n" + "=" * 60)
        print("ğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        print(f"  ë¶„ì„ ëŒ€ìƒ íšŒì‚¬: {total_companies}ê°œ")
        print(f"  ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ëœ íšŒì‚¬: {len(analyzed_selectors)}ê°œ")
        print(f"  ì‹¤íŒ¨í•œ íšŒì‚¬: {total_companies - len(analyzed_selectors)}ê°œ")
        
        if analyzed_selectors:
            print(f"\nğŸ¯ ë°œê²¬ëœ ì„ íƒì ì˜ˆì‹œ:")
            for company, selector in list(analyzed_selectors.items())[:5]:
                print(f"    - {company}: {selector}")
        
        print(f"\nğŸ‰ ì„ íƒìê°€ ì—†ë˜ íšŒì‚¬ë“¤ì˜ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

def main():
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    config_manager = ConfigManager(
        os.path.join(base_dir, 'data', 'ì±„ìš©ê³µê³ _ëª©ë¡.csv'), 
        os.path.join(base_dir, 'html')
    )
    if not config_manager.update_selectors():
        print("\nâŒ ì¼ë¶€ ì‘ì—…ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()