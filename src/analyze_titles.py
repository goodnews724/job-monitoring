import os
import pandas as pd
from bs4 import BeautifulSoup
from collections import Counter
import re
from typing import List, Tuple, Optional, Dict, Set
import logging

class JobPostingSelectorAnalyzer:
    """ì±„ìš©ê³µê³  ì„ íƒì ë¶„ì„ê¸° (ê³µìœ  ìºì‹œ ê¸°ëŠ¥ ì¶”ê°€)"""
    
    def __init__(self):
        self.job_keywords = [
            'ê°œë°œì', 'ì—”ì§€ë‹ˆì–´', 'ë””ìì´ë„ˆ', 'ê¸°íšì', 'ë§¤ë‹ˆì €', 'PM', 'íŒ€ì¥', 'ì „ë¬¸ê°€', 'ë‹´ë‹¹ì',
            'developer', 'engineer', 'designer', 'manager', 'planner', 'lead', 'specialist', 'analyst',
            # ì§ë¬´ ë¶„ì•¼ ì¶”ê°€
            'ê°œë°œ', 'ì‚¬ì—…', 'R&D', 'ì—°êµ¬', 'ê¸°ìˆ ', 'ë°ì´í„°', 'ë³´ì•ˆ', 'AI', 'ì¸ê³µì§€ëŠ¥', 'í´ë¼ìš°ë“œ',
            'ë§ˆì¼€íŒ…', 'ì˜ì—…', 'ìš´ì˜', 'ì¸ì‚¬', 'ì¬ë¬´', 'ë²•ë¬´', 'UX', 'UI', 'í’ˆì§ˆ', 'QA',
            'ì»¨ì„¤íŒ…', 'ì†”ë£¨ì…˜', 'ì„œë¹„ìŠ¤', 'í”Œë«í¼', 'ì‹œìŠ¤í…œ', 'ë„¤íŠ¸ì›Œí¬', 'ì¸í”„ë¼',
            # ì±„ìš© ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€
            'ì±„ìš©', 'ì¸í„´', 'ì²´í—˜í˜•', 'ìˆ˜ì‹œì±„ìš©', 'ì‹ ì…', 'ê²½ë ¥', 'ì •ê·œì§', 'ê³„ì•½ì§',
            'BM', 'ë””ìì¸', 'ìƒì‚°ì§', 'ì¶©í¬ì¥', 'NEOPHARM'
        ]
        self.position_keywords = ['ì‹ ì…', 'ê²½ë ¥', 'ì¸í„´', 'ì •ê·œì§', 'ê³„ì•½ì§']
        self.exclude_patterns = [
            r'^ê²½ë ¥\s*\d+', r'^ì‹ ì…$', r'^ì¸í„´$', r'\d+ë…„\s*ì´ìƒ', r'\d+ë…„\s*ì´í•˜',
            r'ì±„ìš©\s*í”„ë¡œì„¸ìŠ¤', r'ì§€ì›\s*ë°©ë²•', r'ë³µë¦¬í›„ìƒ', r'ê¸°íƒ€\s*ì‚¬í•­',
            r'ê´€ê³„ì‚¬\s*ì„ íƒ', r'ì „ì²´\s*ì„ íƒ', r'ì´ˆê¸°í™”', r'ë‹«ê¸°', r'Contact',
            r'^[ê°€-í£]+íŒ€$', r'^[ê°€-í£]+ë³¸ë¶€$', r'^[ê°€-í£]+ë¶€ë¬¸$'
        ]
        self.blacklist = ['nav', 'footer', 'header', 'menu', 'sitemap', 'aside', 'sidebar', 'breadcrumb']
        self.selector_blacklist = [
            'div.ViewFooterLink_link-text__yC2ls',  # í‘¸í„° ë§í¬
            'p.official__apply__company__title',  # HDí˜„ëŒ€ íšŒì‚¬ëª… ëª©ë¡ (ì±„ìš©ê³µê³  ì•„ë‹˜)
            'footer',
            'nav',
            '.footer',
            '.navbar',
            '.breadcrumb',
            '.copyright',
            '.privacy',
            '.terms'
        ]
        # DOM êµ¬ì¡° ê¸°ë°˜ í•„í„°ë§ì„ ìœ„í•œ ì˜ì—­ ì •ì˜
        self.excluded_sections = ['nav', 'header', 'footer', 'aside', 'sidebar']
        self.ui_element_patterns = [
            r'(blog|youtube|facebook|instagram|twitter|linkedin)',  # ì†Œì…œë¯¸ë””ì–´
            r'(ë°”ë¡œê°€ê¸°|more|view|link|copy|share|ê³µìœ )',  # ë²„íŠ¼/ë§í¬ í…ìŠ¤íŠ¸
            r'(faq|q&a|notice|contact|about|ë¬¸ì˜|ê³µì§€)',  # ì •ë³´ì„± ë©”ë‰´
            r'^(talent|careers?|culture|people)$',  # ì±„ìš© ê´€ë ¨ ë©”ë‰´ëª…
        ]
        self.logger = self._setup_logger()

    def _setup_logger(self) -> logging.Logger:
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        return logger

    def _is_potential_job_posting(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ê°€ ì±„ìš©ê³µê³ ì¼ ê°€ëŠ¥ì„±ì„ íŒë‹¨í•©ë‹ˆë‹¤."""
        # ê¸¸ì´ ì²´í¬ - í•œêµ­ì–´ ì±„ìš©ê³µê³ ëŠ” ì§§ì„ ìˆ˜ ìˆìŒ
        if not (3 <= len(text) <= 150):  # ë” ì§§ì€ í…ìŠ¤íŠ¸ë„ í—ˆìš© (R&D ê°™ì€)
            return False

        # íšŒì‚¬ëª… íŒ¨í„´ ì œì™¸ - HDí˜„ëŒ€ ê°™ì€ ì¼€ì´ìŠ¤ ëŒ€ì‘
        company_name_patterns = [
            r'^(ê±´ì„¤ê¸°ê³„|HDí˜„ëŒ€\w*|HD\w+|í˜„ëŒ€\w*|ì‚¼ì„±\w*|LG\w*|SK\w*|í¬ìŠ¤ì½”\w*|ë¡¯ë°\w*|í•œí™”\w*|KT\w*|ë„¤ì´ë²„\w*|ì¹´ì¹´ì˜¤\w*|ìš°ì•„í•œ\w*|ë°°ë‹¬ì˜ë¯¼ì¡±\w*|ì¿ íŒ¡\w*|í† ìŠ¤\w*)$',  # íšŒì‚¬ëª…
            r'^(\w+ê·¸ë£¹|\w+í™€ë”©ìŠ¤|\w+ê³„ì—´|\w+ê·¸ë£¹ì‚¬|\w+ì§€ì£¼)$',  # ê·¸ë£¹ì‚¬ëª…
            r'^(\w+\s*ì£¼ì‹íšŒì‚¬|\w+\s*\(ì£¼\)|\w+\s*inc\.?|\w+\s*corp\.?|\w+\s*co\.?|\w+\s*ltd\.?)$',  # ë²•ì¸ëª…
            r'^(\w+ì˜¤ì¼ë±…í¬|\w+í…|\w+ì†”ë£¨ì…˜|\w+ì‹œìŠ¤í…œ|\w+ì—”ì§€ë‹ˆì–´ë§|\w+ì†Œí”„íŠ¸|\w+í…Œí¬|\w+ë©|\w+ìŠ¤íŠœë””ì˜¤)$',  # ì‚¬ì—…ì²´ëª…
        ]

        for pattern in company_name_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return False

        # ì¶”ê°€ í•„í„°ë§: ë‚ ì§œ/ì‹œê°„/íƒœê·¸/ìƒíƒœ íŒ¨í„´ë“¤
        datetime_and_ui_patterns = [
            r'^\d{4}\.\d{2}\.\d{2}\s+\d{2}:\d{2}.*$',  # ë‚ ì§œì‹œê°„ íŒ¨í„´
            r'^D-\d+.*$',  # D-8, D-1 ê°™ì€ íŒ¨í„´
            r'^d-\d+.*$',  # d-8, d-1 ê°™ì€ íŒ¨í„´ (ì†Œë¬¸ì)
            r'^D-\d+$|^D-DAY$',  # ë‚¨ì€ ì¼ìˆ˜ íŒ¨í„´
            r'^#[^#]+(\s+ì™¸\s*\d+)?$',  # íƒœê·¸ íŒ¨í„´ (#ë¶„ë‹¹(GRC) ì™¸ 10)
            r'^[ê°€-í£]+\s*ë“±$',  # "ì¡°ì„ í•´ì–‘ ë“±" íŒ¨í„´
            r'^[ê°€-í£]{2,6}$',  # ì§§ì€ í•œê¸€ ë‹¨ì–´ (íšŒì‚¬ëª…/ë¶€ì„œëª…)
            r'^\d+\s*ì™¸\s*\d+$',  # "10 ì™¸ 3" íŒ¨í„´
        ]

        for pattern in datetime_and_ui_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return False

        # ì œì™¸ íŒ¨í„´ ì²´í¬ - UI ìš”ì†Œë“¤ ì¶”ê°€
        ui_elements = [
            r'^(ê²½ë ¥\s*\d+.*ë…„|ì‹ ì…|ì¸í„´|ì •ê·œì§|ê³„ì•½ì§|ì¸ì¬í’€)$',  # í•„í„° í•­ëª©ë“¤
            r'^(ì±„ìš©\s*FAQ|ì§€ì›ì„œ\s*ì‘ì„±|ë§ˆì´í˜ì´ì§€|ê³µì§€ì‚¬í•­|í™ë³´ì˜ìƒ|CEO\s*ë©”ì‹œì§€|ì±„ìš©\s*Q&A|ì§€ì›ì„œ\s*ìˆ˜ì •|ì¸ì¬ìƒ|ê²½ì˜ì´ë…|ì¸ì¬ìœ¡ì„±|ë³µë¦¬í›„ìƒ)$',  # ë©”ë‰´ í•­ëª©ë“¤
            r'^(ììœ¨\s*ë³µì¥|ì ì‹¬ì‹œê°„|ì—°ì°¨|ê°„ì‹|ê±´ê°•ê²€ì§„|ë™í˜¸íšŒ|ì¸ì„¼í‹°ë¸Œ|ì‹œì°¨ì¶œí‡´ê·¼|ì„ì‹ ê¸°|ë§›ìˆëŠ”|1ì‹œê°„|8ì‹œ~10ì‹œ).*$',  # ë³µë¦¬í›„ìƒ
            r'^(DESIGN|SALES|PRODUCT|STORE|STAFF|GLOBAL|CX|SCM|OFF-SALES|CEO\s*STAFF)$',  # ë¶€ì„œëª…ë§Œ
            r'^(Tech|ë§ˆì¼€íŒ…/í™ë³´|ê²½ì˜ì§€ì›|ë””ìì¸|ì½˜í…ì¸ \s*ë¹„ì¦ˆë‹ˆìŠ¤)$',  # ì¹´í…Œê³ ë¦¬ëª…
            r'^\-$',  # ë‹¨ìˆœ ëŒ€ì‹œ
            r'^(ê²½ë ¥\s*ë¬´ê´€|ê²½ë ¥\s*\d+~\d+ë…„|ê²½ë ¥\s*\d+ë…„\s*ì´ìƒ|ë² ë¦¬ì‹œ.*ìŠ¤í† ì–´)$',  # ê¸°íƒ€ í•„í„°ë“¤
            r'^(ê°œì¸ì •ë³´\s*ì²˜ë¦¬ë°©ì¹¨|ì´ìš©ì•½ê´€|ì €ì‘ê¶Œ|All\s*Rights\s*Reserved|Copyright).*$',  # í‘¸í„° ê´€ë ¨
            r'^(ì‚¬ì´íŠ¸ë§µ|Contact\s*Us|ê³ ê°ì„¼í„°|ë¬¸ì˜|ì•ˆë‚´)$',  # ë„¤ë¹„ê²Œì´ì…˜
            r'^(#[ê°€-í£]+|#[ê°€-í£]+\([^)]+\)|#ICT|#ì—°êµ¬|#ì˜ì—…|#ì„¤ê³„|#ìƒì‚°ì§|#ëŒ€ì‚°|#ë¶„ë‹¹).*$',  # í•´ì‹œíƒœê·¸ë“¤
            # ë„¤ë¹„ê²Œì´ì…˜ ë§í¬ íŒ¨í„´ ì¶”ê°€
            r'^(.*\s*ë°”ë¡œê°€ê¸°\s*â‰«?)$',  # "ì±„ìš©ê³µê³  ë°”ë¡œê°€ê¸° â‰«", "ì§ë¬´ì†Œê°œ ë°”ë¡œê°€ê¸°" ë“±
            r'^(.*\s*ë°”ë¡œê°€ê¸°)$',  # "ì±„ìš©ê³µê³  ë°”ë¡œê°€ê¸°" ë“±
            r'^(ì±„ìš©ì •ë³´|ì§ë¬´ì†Œê°œ|ë³µë¦¬í›„ìƒ|ê¸°ì—…ë¬¸í™”|íšŒì‚¬ì†Œê°œ|ì˜¤ì‹œëŠ”ê¸¸|Contact|ë¬¸ì˜í•˜ê¸°)\s*(ë°”ë¡œê°€ê¸°)?.*$',  # ì£¼ìš” ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ë“¤
        ]

        # ìƒˆë¡œ ì¶”ê°€ëœ UI ìš”ì†Œ íŒ¨í„´ë“¤ ì ìš©
        for pattern in self.ui_element_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return False

        for pattern in self.exclude_patterns + ui_elements:
            if re.search(pattern, text, re.IGNORECASE):
                return False
        
        # ì§ë¬´ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
        has_job_keyword = any(keyword in text for keyword in self.job_keywords)
        
        # ì§ë¬´ëª… + ì§ì±… íŒ¨í„´ (ì˜ˆ: "ë°±ì—”ë“œ ê°œë°œì", "UI/UX ë””ìì´ë„ˆ") 
        job_title_pattern = r'(ë°±ì—”ë“œ|í”„ë¡ íŠ¸ì—”ë“œ|í’€ìŠ¤íƒ|ëª¨ë°”ì¼|ì›¹|UI/UX|ê·¸ë˜í”½|ë¸Œëœë“œ|ë§ˆì¼€íŒ…|ë°ì´í„°|AI|ML|DevOps|QA|ê¸°íš|ìš´ì˜|ì˜ì—…|HR|ì¬ë¬´|ë²•ë¬´|ê°œë°œì|ë””ìì´ë„ˆ|ì—”ì§€ë‹ˆì–´|ë§¤ë‹ˆì €|ê¸°íšì|ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸)'
        has_job_pattern = bool(re.search(job_title_pattern, text))
        
        # ìµœì†Œ ì¡°ê±´: ì§ë¬´ ê´€ë ¨ í‚¤ì›Œë“œë‚˜ íŒ¨í„´ì´ ìˆì–´ì•¼ í•¨
        # ë˜ëŠ” ì±„ìš© í˜ì´ì§€/ì„¹ì…˜ì— ìˆëŠ” ì§§ì€ í…ìŠ¤íŠ¸ë“¤ë„ í—ˆìš© (3-15ê¸€ì)
        if not (has_job_keyword or has_job_pattern):
            # ì§§ì€ í…ìŠ¤íŠ¸ì´ê³  í•œê¸€+ì˜ë¬¸+&ê¸°í˜¸ë§Œ í¬í•¨ëœ ê²½ìš° í—ˆìš© (ì§ë¬´ëª…ì¼ ê°€ëŠ¥ì„±)
            if len(text) <= 15 and re.match(r'^[ê°€-í£a-zA-Z&\s]+$', text.strip()):
                pass  # í—ˆìš©
            else:
                return False
            
        # ë‹¨ìˆœí•œ íŒ€ëª…ì´ë‚˜ ë¶€ì„œëª… ì œì™¸ (ëì— 'íŒ€', 'ë³¸ë¶€', 'ë¶€ë¬¸'ìœ¼ë¡œ ëë‚˜ëŠ” ê²ƒë“¤)
        if re.search(r'^[\ê°€-í£]+íŒ€$|^[\ê°€-í£]+ë³¸ë¶€$|^[\ê°€-í£]+ë¶€ë¬¸$', text):
            return False
            
        return True

    def _calculate_job_posting_weight(self, text: str) -> float:
        """ì±„ìš©ê³µê³ ì¼ ê°€ëŠ¥ì„±ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        weight = 1.0
        
        # ì‹¤ì œ ì±„ìš©ê³µê³  ì œëª© íŒ¨í„´ë“¤ì— ë†’ì€ ê°€ì¤‘ì¹˜
        job_title_patterns = [
            r'.*(ê°œë°œì|ì—”ì§€ë‹ˆì–´|ë””ìì´ë„ˆ|ê¸°íšì|ë§¤ë‹ˆì €|ì „ë¬¸ê°€|ë‹´ë‹¹ì|íŒ€ì¥|ë¶€ì¥|ëŒ€ë¦¬|ê³¼ì¥|ì£¼ì„)',
            r'.*(ë°±ì—”ë“œ|í”„ë¡ íŠ¸ì—”ë“œ|í’€ìŠ¤íƒ|ëª¨ë°”ì¼|ì›¹|UI/UX|DevOps|QA)',
            r'.*(ì‹œë‹ˆì–´|ì£¼ë‹ˆì–´|ì‹ ì…|ê²½ë ¥).*ì±„ìš©',
            r'(í”„ë¡œë•íŠ¸|ì„œë¹„ìŠ¤|ë¹„ì¦ˆë‹ˆìŠ¤|ì½˜í…ì¸ ).*\s+(ê¸°íš|ê°œë°œ|ìš´ì˜)',
            r'.*ì±„ìš©.*\s+(ì „í˜•|ê³µê³ )',  # "ì‹ ì…ì‚¬ì› ì±„ìš© I'Mì „í˜•" ê°™ì€ íŒ¨í„´
            r'.*\s+(ì¸í„´|ì²´í—˜í˜•).*',    # ì¸í„´ ì±„ìš©
        ]
        
        for pattern in job_title_patterns:
            if re.search(pattern, text):
                weight += 3.0  # ê°€ì¤‘ì¹˜ ì¦ê°€
                break
        
        # íšŒì‚¬ëª…ì´ í¬í•¨ëœ ê²½ìš° (ì˜ˆ: [NEOPHARM], [SNOW])
        if re.search(r'\[.*\]', text):
            weight += 2.0
            
        # ì—°ë„ê°€ í¬í•¨ëœ ê²½ìš° (ì˜ˆ: 2025ë…„)
        if re.search(r'20\d{2}ë…„', text):
            weight += 2.0
        
        # íšŒì‚¬ëª…ì´ë‚˜ êµ¬ì²´ì ì¸ ì§ë¬´ê°€ í¬í•¨ëœ ê²½ìš°
        if any(keyword in text for keyword in ['íŒ€', 'ì„¼í„°', 'ë³¸ë¶€', 'ì‚¬ì—…ë¶€']):
            weight += 1.0
            
        # ê¸¸ì´ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ - ì±„ìš©ê³µê³  ì œëª©ì€ ë³´í†µ ê¸¸ë‹¤
        if 15 <= len(text) <= 80:
            weight += 2.0
        elif 10 <= len(text) < 15:
            weight += 1.0
        elif len(text) < 8:
            weight -= 1.0
            
        return weight

    def _filter_valid_job_titles(self, titles: List[str]) -> List[str]:
        """ì±„ìš©ê³µê³  ì œëª©ì„ í•„í„°ë§í•©ë‹ˆë‹¤."""
        valid_titles = []
        for title in titles:
            if self._is_potential_job_posting(title):
                # ì¤‘ë³µ ì œê±° (ìœ ì‚¬í•œ ì œëª©ë“¤)
                if not any(self._is_similar_title(title, existing) for existing in valid_titles):
                    valid_titles.append(title)
        return valid_titles

    def _is_similar_title(self, title1: str, title2: str) -> bool:
        """ë‘ ì œëª©ì´ ìœ ì‚¬í•œì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
        # ê¸¸ì´ê°€ ë§¤ìš° ë‹¤ë¥´ë©´ ë‹¤ë¥¸ ì œëª©
        if abs(len(title1) - len(title2)) > min(len(title1), len(title2)) * 0.5:
            return False
        
        # ê³µí†µ ë‹¨ì–´ ë¹„ìœ¨ ì²´í¬
        words1 = set(title1.split())
        words2 = set(title2.split())
        common_words = words1.intersection(words2)
        
        if not common_words:
            return False
        
        similarity = len(common_words) / max(len(words1), len(words2))
        return similarity > 0.7

    def _validate_selector(self, soup: BeautifulSoup, selector: str) -> Optional[List[str]]:
        """ê¸°ì¡´ ì„ íƒìì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤."""
        try:
            elements = soup.select(selector)
            if not elements:
                return None
            
            titles = [elem.get_text(strip=True) for elem in elements]
            valid_titles = self._filter_valid_job_titles(titles)
            
            if len(valid_titles) >= 2:
                return valid_titles
            else:
                return None
        except Exception:
            return None

    def _find_job_posting_containers(self, soup: BeautifulSoup) -> List[Tuple[str, List[str]]]:
        """ì±„ìš©ê³µê³  ì „ìš© ì»¨í…Œì´ë„ˆë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
        job_container_patterns = [
            # ID ê¸°ë°˜ íŒ¨í„´
            r'job|recruit|career|position|opening|notice|list',
            # í´ë˜ìŠ¤ ê¸°ë°˜ íŒ¨í„´
            r'job|recruit|career|position|opening|notice|list|bbs'
        ]

        potential_containers = []

        # 1. ID ê¸°ë°˜ ì»¨í…Œì´ë„ˆ ìš°ì„  ê²€ìƒ‰
        for element in soup.find_all(attrs={'id': True}):
            element_id = element.get('id', '').lower()
            if any(re.search(pattern, element_id, re.IGNORECASE) for pattern in job_container_patterns):
                # ì´ ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ ì±„ìš©ê³µê³  ë§í¬ ì°¾ê¸°
                job_links = []
                for link in element.find_all('a', href=True):
                    text = link.get_text(strip=True)
                    if self._is_potential_job_posting(text) and len(text) > 10:
                        job_links.append(text)

                if len(job_links) >= 1:  # ìµœì†Œ 1ê°œ ì´ìƒ
                    # ë” êµ¬ì²´ì ì¸ ì„ íƒì ìƒì„±
                    specific_selectors = self._generate_specific_selectors(element, job_links, soup)
                    for selector, titles in specific_selectors:
                        potential_containers.append((selector, titles))

        # 2. í´ë˜ìŠ¤ ê¸°ë°˜ ì»¨í…Œì´ë„ˆ ê²€ìƒ‰
        for element in soup.find_all(attrs={'class': True}):
            classes = ' '.join(element.get('class', [])).lower()
            if any(re.search(pattern, classes, re.IGNORECASE) for pattern in job_container_patterns):
                # ì´ ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ ì±„ìš©ê³µê³  ë§í¬ ì°¾ê¸°
                job_links = []
                for link in element.find_all('a', href=True):
                    text = link.get_text(strip=True)
                    if self._is_potential_job_posting(text) and len(text) > 10:
                        job_links.append(text)

                if len(job_links) >= 1:
                    specific_selectors = self._generate_specific_selectors(element, job_links, soup)
                    for selector, titles in specific_selectors:
                        potential_containers.append((selector, titles))

        return potential_containers

    def _generate_specific_selectors(self, container, job_links: List[str], soup: BeautifulSoup) -> List[Tuple[str, List[str]]]:
        """ì»¨í…Œì´ë„ˆ ë‚´ì—ì„œ êµ¬ì²´ì ì¸ ì„ íƒìë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        selectors = []

        # ì»¨í…Œì´ë„ˆ ì„ íƒì ìƒì„±
        container_selector = self._get_element_selector(container)

        # ì±„ìš©ê³µê³  ë§í¬ë“¤ì˜ ê³µí†µ ê²½ë¡œ ì°¾ê¸°
        job_link_elements = []
        for link in container.find_all('a', href=True):
            text = link.get_text(strip=True)
            if text in job_links:
                job_link_elements.append(link)

        if not job_link_elements:
            return selectors

        # ê³µí†µ ë¶€ëª¨-ìì‹ ê´€ê³„ íŒ¨í„´ ì°¾ê¸°
        common_patterns = self._find_common_path_patterns(job_link_elements, container)

        for pattern in common_patterns:
            full_selector = f"{container_selector} {pattern}"
            try:
                # ì„ íƒì ê²€ì¦
                elements = soup.select(full_selector)
                titles = [elem.get_text(strip=True) for elem in elements]
                # ì±„ìš©ê³µê³ ë§Œ í•„í„°ë§
                valid_titles = [t for t in titles if self._is_potential_job_posting(t) and len(t) > 10]

                if len(valid_titles) >= 1:
                    selectors.append((full_selector, valid_titles))
            except Exception:
                continue

        return selectors

    def _get_element_selector(self, element) -> str:
        """ìš”ì†Œì˜ CSS ì„ íƒìë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if element.get('id'):
            return f"#{element.get('id')}"
        elif element.get('class'):
            classes = '.'.join(element.get('class'))
            return f"{element.name}.{classes}"
        else:
            return element.name

    def _find_common_path_patterns(self, elements: List, container) -> List[str]:
        """ìš”ì†Œë“¤ì˜ ê³µí†µ ê²½ë¡œ íŒ¨í„´ì„ ì°¾ìŠµë‹ˆë‹¤."""
        patterns = set()

        for element in elements:
            # ìš”ì†Œê¹Œì§€ì˜ ê²½ë¡œ ìƒì„±
            path_parts = []
            current = element

            while current and current != container and current.parent:
                if current.name == 'a':
                    path_parts.append('a')
                elif current.get('class'):
                    classes = '.'.join(current.get('class'))
                    path_parts.append(f"{current.name}.{classes}")
                else:
                    path_parts.append(current.name)
                current = current.parent

            if path_parts:
                path_parts.reverse()
                patterns.add(' '.join(path_parts))

        return list(patterns)

    def find_best_selector(self, soup: BeautifulSoup) -> Tuple[Optional[str], List[str]]:
        # 1. ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ ìš°ì„  ì ìš©: êµ¬ì²´ì ì¸ ì±„ìš©ê³µê³  ì»¨í…Œì´ë„ˆ ì°¾ê¸°
        specific_containers = self._find_job_posting_containers(soup)
        if specific_containers:
            # ê°€ì¥ ë§ì€ ì±„ìš©ê³µê³ ë¥¼ ì°¾ì€ ì„ íƒì ë°˜í™˜
            best_container = max(specific_containers, key=lambda x: len(x[1]))
            if len(best_container[1]) >= 1:
                return best_container[0], best_container[1]

        # 2. ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í´ë°±
        # ë¨¼ì € ë§í¬ í…ìŠ¤íŠ¸ë¥¼ ìš°ì„ ìœ¼ë¡œ ê²€ì‚¬
        link_elements = []
        for link in soup.find_all('a', href=True):
            if any(parent.name in self.blacklist or any(b in (parent.get('class', []) + ([parent.get('id')] if parent.get('id') else [])) for b in self.blacklist) for parent in link.find_parents()):
                continue

            text = link.get_text(strip=True)
            if self._is_potential_job_posting(text) and len(text) > 5:  # ë§í¬ ì¡°ê±´ ì™„í™”
                link_elements.append(link)
        
        # ë§í¬ì—ì„œ ì¶©ë¶„í•œ ì±„ìš©ê³µê³ ë¥¼ ì°¾ì•˜ìœ¼ë©´ ë§í¬ ìš°ì„  ì‚¬ìš©
        if len(link_elements) >= 2:
            potential_elements = link_elements
        else:
            # 2. ì¼ë°˜ ìš”ì†Œë“¤ë„ ê²€ì‚¬
            potential_elements = []
            for element in soup.find_all(['p', 'div', 'span', 'a', 'strong', 'h2', 'h3', 'h4', 'li', 'dt', 'td']):
                if any(parent.name in self.blacklist or any(b in (parent.get('class', []) + ([parent.get('id')] if parent.get('id') else [])) for b in self.blacklist) for parent in element.find_parents()):
                    continue
                
                text = element.get_text(strip=True)
                if self._is_potential_job_posting(text):
                    potential_elements.append(element)
            
            # ë§í¬ ìš”ì†Œë“¤ë„ ì¶”ê°€
            potential_elements.extend(link_elements)

        if len(potential_elements) < 2:
            return None, []

        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ë¶€ëª¨ ì»¨í…Œì´ë„ˆ ì„ íƒ
        parent_scores = {}
        for element in potential_elements:
            text = element.get_text(strip=True)
            weight = self._calculate_job_posting_weight(text)
            
            for parent in element.find_parents(limit=3):
                if parent not in parent_scores:
                    parent_scores[parent] = 0
                parent_scores[parent] += weight
        
        if not parent_scores:
            return None, []

        # ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ì»¨í…Œì´ë„ˆ ì„ íƒ
        container = max(parent_scores.items(), key=lambda x: x[1])[0]

        child_selector_counter = Counter()
        for element in potential_elements:
            if container in element.find_parents():
                # ë” êµ¬ì²´ì ì¸ ì„ íƒì ìƒì„±
                tag = element.name
                classes = element.get('class', [])
                element_id = element.get('id', '')

                # êµ¬ì²´ì ì¸ ì„ íƒì ë§Œë“¤ê¸°
                if element_id:
                    # IDê°€ ìˆìœ¼ë©´ ID ì‚¬ìš©
                    selector_key = f"{tag}#{element_id}"
                elif classes:
                    # í´ë˜ìŠ¤ê°€ ìˆìœ¼ë©´ í´ë˜ìŠ¤ ì‚¬ìš©
                    class_str = '.' + '.'.join(sorted(classes))
                    selector_key = f"{tag}{class_str}"
                else:
                    # í´ë˜ìŠ¤ë‚˜ IDê°€ ì—†ìœ¼ë©´ ë¶€ëª¨ì™€ì˜ ê´€ê³„ë¡œ êµ¬ì²´í™”
                    parent = element.parent
                    if parent and parent != container:
                        parent_tag = parent.name
                        parent_classes = parent.get('class', [])
                        if parent_classes:
                            parent_class_str = '.' + '.'.join(sorted(parent_classes))
                            selector_key = f"{parent_tag}{parent_class_str} > {tag}"
                        else:
                            selector_key = f"{parent_tag} > {tag}"
                    else:
                        # ìµœí›„ì˜ ìˆ˜ë‹¨: íƒœê·¸ëª… + ì†ì„±
                        attrs = []
                        if element.get('href'):
                            attrs.append('[href]')
                        if element.get('title'):
                            attrs.append('[title]')
                        if attrs:
                            selector_key = f"{tag}{''.join(attrs)}"
                        else:
                            # ì •ë§ ë§ˆì§€ë§‰ì—ë§Œ íƒœê·¸ëª…ë§Œ ì‚¬ìš©
                            selector_key = tag

                child_selector_counter[selector_key] += 1

        if not child_selector_counter:
            return None, []

        # ê°€ì¥ êµ¬ì²´ì ì¸ ì„ íƒì ìš°ì„  ì„ íƒ
        best_child_selector = None
        for selector, count in child_selector_counter.most_common():
            # ë‹¨ìˆœ íƒœê·¸ëª…ì€ í”¼í•˜ê³  ë” êµ¬ì²´ì ì¸ ê²ƒ ì„ íƒ
            if ('.' in selector or '#' in selector or '>' in selector or '[' in selector):
                best_child_selector = selector
                break

        # êµ¬ì²´ì ì¸ ì„ íƒìê°€ ì—†ìœ¼ë©´ ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ê²ƒ ì‚¬ìš©
        if not best_child_selector:
            best_child_selector = child_selector_counter.most_common(1)[0][0]
        
        container_tag = container.name
        container_classes = '.' + '.'.join(sorted(container.get('class', []))) if container.get('class') else ''
        
        final_selector_candidates = Counter()
        try:
            for element in soup.select(f"{container_tag}{container_classes} > {best_child_selector}"):
                for p_elem in element.find_all(['p', 'strong', 'h2', 'h3', 'h4', 'div']):
                     if p_elem.get('class'):
                         cls = '.' + '.'.join(p_elem.get('class'))
                         final_selector_candidates[f"{best_child_selector} {p_elem.name}{cls}"] += 1
        except Exception:
            pass

        if not final_selector_candidates:
             final_selector = best_child_selector
        else:
            best_selector_str = None
            max_score = -1
            for selector, count in final_selector_candidates.most_common():
                if count < 2: continue
                try:
                    texts = [elem.get_text(strip=True) for elem in soup.select(selector)]
                    if not texts: continue
                    
                    valid_texts = [t for t in texts if 10 < len(t) < 150]
                    if len(valid_texts) < 2: continue

                    avg_len = sum(len(t) for t in valid_texts) / len(valid_texts)
                    
                    keyword_score = sum(1 for t in valid_texts if any(re.search(k, t, re.IGNORECASE) for k in self.job_keywords))
                    keyword_ratio = keyword_score / len(valid_texts)
                    
                    score = avg_len * keyword_ratio

                    if score > max_score:
                        max_score = score
                        best_selector_str = selector
                except Exception:
                    continue
            
            if best_selector_str:
                final_selector = best_selector_str
            else:
                final_selector = final_selector_candidates.most_common(1)[0][0]

        # ìµœì¢… ì„ íƒì ê²€ì¦ ë° ë°˜í™˜
        def is_specific_selector(selector):
            """ì„ íƒìê°€ ì¶©ë¶„íˆ êµ¬ì²´ì ì¸ì§€ í™•ì¸"""
            return ('.' in selector or '#' in selector or '>' in selector or '[' in selector or ':' in selector)

        def is_blacklisted_selector(selector):
            """ì„ íƒìê°€ ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸"""
            # ê¸°ë³¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸
            for blacklisted in self.selector_blacklist:
                if blacklisted in selector:
                    return True

            # ë„ˆë¬´ ì¼ë°˜ì ì¸ ì„ íƒìë“¤ ê¸ˆì§€
            generic_selectors = [
                'a[href]',  # ëª¨ë“  ë§í¬
                'a',        # ëª¨ë“  ë§í¬
                'div > a',  # ë„ˆë¬´ ì¼ë°˜ì 
                'li > a',   # ë„ˆë¬´ ì¼ë°˜ì  (í•˜ì§€ë§Œ êµ¬ì²´ì ì¸ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í—ˆìš©)
                'td > a',   # ë„ˆë¬´ ì¼ë°˜ì 
                'p > a',    # ë„ˆë¬´ ì¼ë°˜ì 
                'tr > td',  # ë„ˆë¬´ ì¼ë°˜ì 
                'ul > li',  # ë„ˆë¬´ ì¼ë°˜ì 
                'div > p',  # ë„ˆë¬´ ì¼ë°˜ì 
                'li',       # ë„ˆë¬´ ì¼ë°˜ì 
                'td',       # ë„ˆë¬´ ì¼ë°˜ì 
                'dt',       # ë„ˆë¬´ ì¼ë°˜ì 
            ]

            # ë‹¨ìˆœíˆ íƒœê·¸ë§Œ ìˆëŠ” ì„ íƒìëŠ” ê¸ˆì§€
            if selector.strip() in generic_selectors:
                return True

            return False

        def clean_selector(selector):
            """ì„ íƒìì—ì„œ ë§ˆì§€ë§‰ ìš”ì†Œì˜ nth-childë§Œ ì œê±°í•˜ì—¬ ì „ì²´ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆê²Œ í•¨"""
            # ì„ íƒìë¥¼ > ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
            parts = selector.split(' > ')
            if parts:
                # ë§ˆì§€ë§‰ ìš”ì†Œì—ì„œë§Œ nth-child/nth-of-type ì œê±°
                parts[-1] = re.sub(r':nth-child\(\d+\)', '', parts[-1])
                parts[-1] = re.sub(r':nth-of-type\(\d+\)', '', parts[-1])
                cleaned = ' > '.join(parts)
            else:
                # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ëœ ê²½ìš° (> ì—†ëŠ” ê²½ìš°)
                parts = selector.split()
                if parts:
                    parts[-1] = re.sub(r':nth-child\(\d+\)', '', parts[-1])
                    parts[-1] = re.sub(r':nth-of-type\(\d+\)', '', parts[-1])
                    cleaned = ' '.join(parts)
                else:
                    cleaned = selector

            # ì—°ì†ëœ ê³µë°± ì •ë¦¬
            cleaned = re.sub(r'\s+', ' ', cleaned).strip()
            # > ì•ë’¤ ê³µë°± ì •ë¦¬
            cleaned = re.sub(r'\s*>\s*', ' > ', cleaned)
            return cleaned

        def validate_job_selector(selector, soup):
            """ì„ íƒìê°€ ì‹¤ì œ ì±„ìš©ê³µê³ ë¥¼ ê°€ì ¸ì˜¤ëŠ”ì§€ ê²€ì¦"""
            try:
                elements = soup.select(selector)
                if not elements:
                    return None, []

                titles = [elem.get_text(strip=True) for elem in elements]
                valid_titles = [t for t in titles if len(t) > 5]

                if len(valid_titles) >= 2:
                    # ì±„ìš©ê³µê³  ê´€ë ¨ì„± ê²€ì‚¬
                    job_related_count = sum(1 for t in valid_titles if self._is_potential_job_posting(t))
                    if job_related_count >= 1:  # ìµœì†Œ 1ê°œ ì´ìƒì˜ ì±„ìš©ê³µê³ 
                        return selector, valid_titles

                return None, []
            except Exception:
                return None, []

        # 1. final_selector ê²€ì¦
        if final_selector and is_specific_selector(final_selector) and not is_blacklisted_selector(final_selector):
            cleaned_final = clean_selector(final_selector)
            result_selector, result_titles = validate_job_selector(cleaned_final, soup)
            if result_selector:
                return result_selector, result_titles

        # 2. best_child_selector ê²€ì¦
        if best_child_selector and is_specific_selector(best_child_selector) and not is_blacklisted_selector(best_child_selector):
            cleaned_best = clean_selector(best_child_selector)
            result_selector, result_titles = validate_job_selector(cleaned_best, soup)
            if result_selector:
                return result_selector, result_titles

        # êµ¬ì²´ì ì¸ ì„ íƒìë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° None ë°˜í™˜
        return None, []

class ConfigManager:
    def __init__(self, config_path: str, html_dir: str):
        self.config_path = config_path
        self.html_dir = html_dir
        self.analyzer = JobPostingSelectorAnalyzer()
        self.logger = logging.getLogger(__name__)
        self.verified_selector_cache: Set[str] = set()
    
    def update_selectors(self) -> bool:
        try:
            df_config = pd.read_csv(self.config_path)
        except FileNotFoundError:
            self.logger.error(f"ì„¤ì • íŒŒì¼ '{self.config_path}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        # ì„ íƒìê°€ ì—†ëŠ” íšŒì‚¬ë§Œ í•„í„°ë§
        companies_without_selector = df_config[
            df_config['selector'].isna() | 
            (df_config['selector'].str.strip() == '') |
            df_config['selector'].isnull()
        ]
        
        if companies_without_selector.empty:
            print("=" * 60)
            print("ëª¨ë“  íšŒì‚¬ì— ì´ë¯¸ ì„ íƒìê°€ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            print("=" * 60)
            return True
        
        print("=" * 60)
        print(f"ì„ íƒìê°€ ì—†ëŠ” {len(companies_without_selector)}ê°œ íšŒì‚¬ ë¶„ì„ ì‹œì‘")
        print("=" * 60)
        
        analyzed_selectors = self._analyze_html_files(companies_without_selector)
        
        # ê²°ê³¼ë¥¼ ì›ë³¸ DataFrameì— ë°˜ì˜
        for company_name, selector in analyzed_selectors.items():
            df_config.loc[df_config['íšŒì‚¬ëª…'] == company_name, 'selector'] = selector

        self._save_config(df_config)
        self._print_summary(analyzed_selectors, len(companies_without_selector))
        return True

    def _analyze_html_files(self, companies_df: pd.DataFrame) -> Dict[str, str]:
        analyzed_selectors = {}
        
        for idx, (_, row) in enumerate(companies_df.iterrows(), 1):
            company_name = row['íšŒì‚¬ëª…']
            print(f"\n[{idx}/{len(companies_df)}] {company_name} ë¶„ì„ ì¤‘...")
            
            html_file_path = os.path.join(self.html_dir, f"{company_name}.html")
            if not os.path.exists(html_file_path):
                print(f"  âŒ HTML íŒŒì¼ì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            
            try:
                with open(html_file_path, 'r', encoding='utf-8') as f:
                    soup = BeautifulSoup(f, 'html.parser')
                
                # 1ë‹¨ê³„: ì´ë²ˆ ì‹¤í–‰ì—ì„œ ê²€ì¦ëœ ê³µìœ  ìºì‹œ í™•ì¸
                is_cached = False
                for cached_selector in self.verified_selector_cache:
                    found_titles = self.analyzer._validate_selector(soup, cached_selector)
                    if found_titles:
                        print(f"  âœ… [ê³µìœ  ìºì‹œ ì ì¤‘] ë‹¤ë¥¸ íšŒì‚¬ì—ì„œ ê²€ì¦ëœ ì„ íƒì '{cached_selector}'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        analyzed_selectors[company_name] = cached_selector
                        for title in found_titles[:3]:
                            print(f"    - {title}")
                        is_cached = True
                        break
                
                if is_cached:
                    continue

                # 2ë‹¨ê³„: ìƒˆë¡œìš´ ë¶„ì„ ì‹¤í–‰
                print("  ğŸ” ìƒˆë¡œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
                best_selector, found_titles = self.analyzer.find_best_selector(soup)
                
                if best_selector and found_titles:
                    print(f"  âœ… ì„±ê³µ! ìƒˆë¡œìš´ ì„ íƒì: '{best_selector}'")
                    analyzed_selectors[company_name] = best_selector
                    self.verified_selector_cache.add(best_selector)
                    for title in found_titles[:3]:
                        print(f"    - {title}")
                else:
                    print("  âŒ ë¶„ì„ ì‹¤íŒ¨: ìœ íš¨í•œ íŒ¨í„´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                self.logger.error(f"  âŒ {company_name} ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                
        return analyzed_selectors

    def _save_config(self, df_config: pd.DataFrame) -> bool:
        try:
            df_config.to_csv(self.config_path, index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ë¥¼ '{self.config_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            return True
        except Exception as e:
            self.logger.error(f"ì„¤ì • íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _print_summary(self, analyzed_selectors: Dict[str, str], total_companies: int):
        print("\n" + "=" * 60)
        print("ğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        print(f"  ë¶„ì„ ëŒ€ìƒ íšŒì‚¬: {total_companies}ê°œ")
        print(f"  ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ëœ íšŒì‚¬: {len(analyzed_selectors)}ê°œ")
        print(f"  ì‹¤íŒ¨í•œ íšŒì‚¬: {total_companies - len(analyzed_selectors)}ê°œ")
        
        if analyzed_selectors:
            print(f"\nğŸ¯ ë°œê²¬ëœ ì„ íƒì ì˜ˆì‹œ:")
            for company, selector in list(analyzed_selectors.items())[:5]:
                print(f"    - {company}: {selector}")
        
        print(f"\nğŸ‰ ì„ íƒìê°€ ì—†ë˜ íšŒì‚¬ë“¤ì˜ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

def main():
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    config_manager = ConfigManager(
        os.path.join(base_dir, 'data', 'ì±„ìš©ê³µê³ _ëª©ë¡.csv'), 
        os.path.join(base_dir, 'html')
    )
    if not config_manager.update_selectors():
        print("\nâŒ ì¼ë¶€ ì‘ì—…ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()