import os
import time
import pandas as pd
import requests
import re
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
from datetime import datetime
import pytz
import logging
from typing import Dict, List, Set, Tuple, Optional
from dotenv import load_dotenv
from google_sheet_utils import GoogleSheetManager
from analyze_titles import JobPostingSelectorAnalyzer
from utils import stabilize_selector, SeleniumRequirementChecker
import concurrent.futures

load_dotenv()

class JobMonitoringDAG:
    def __init__(self, base_dir: str, worksheet_name: str = '[ë“±ë¡]ì±„ìš©í™ˆí˜ì´ì§€ ëª¨ìŒ', webhook_url_env: str = 'SLACK_WEBHOOK_URL', results_filename: str = 'job_postings_latest.csv'):
        self.base_dir = base_dir
        self.data_dir = os.path.join(base_dir, 'data')
        self.html_dir = os.path.join(base_dir, 'html')
        # HTML ë””ë ‰í† ë¦¬ ìƒì„± (ë” ì•ˆì „í•œ ë°©ì‹)
        try:
            os.makedirs(self.html_dir, exist_ok=True)
        except (FileExistsError, OSError):
            # ì´ë¯¸ ì¡´ì¬í•˜ê±°ë‚˜ ê¶Œí•œ ë¬¸ì œê°€ ìˆì–´ë„ ê³„ì† ì§„í–‰
            pass
        self.worksheet_name = worksheet_name
        self.webhook_url_env = webhook_url_env  # í™˜ê²½ë³€ìˆ˜ ì´ë¦„ ì €ì¥
        self.results_path = os.path.join(self.data_dir, results_filename)
        self.webhook_url = os.getenv(webhook_url_env)
        self.company_urls = {}
        self.max_workers = int(os.getenv('MAX_WORKERS', '3'))
        self.foreign_keywords = []  # ì™¸êµ­ì¸ ì±„ìš©ê³µê³  í‚¤ì›Œë“œ

        # requests ì„¸ì…˜ ì„¤ì • (ì¿ í‚¤ ë° ì—°ê²° ìœ ì§€)
        self.session = requests.Session()
        self._setup_session()
        self._setup_logging()

    def _setup_session(self):
        """HTTP ì„¸ì…˜ ì„¤ì • (ë” í˜„ì‹¤ì ì¸ ë¸Œë¼ìš°ì € ëª¨ë°©)"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        }
        self.session.headers.update(headers)

        # HTTP ì–´ëŒ‘í„° ì„¤ì • (ì—°ê²° í’€ë§, ì¬ì‹œë„ ë“±)
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry

        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)

    def _setup_logging(self):
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)

        # ì´ ë¡œê±°ë‚˜ ë£¨íŠ¸ ë¡œê±°ì— í•¸ë“¤ëŸ¬ê°€ ì—†ì„ ë•Œë§Œ í•¸ë“¤ëŸ¬ë¥¼ ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
        if not self.logger.handlers and not logging.getLogger().handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)

    def run(self):
        self.sheet_manager = GoogleSheetManager(self.base_dir)
        self.selenium_checker = SeleniumRequirementChecker()
        self.selector_analyzer = JobPostingSelectorAnalyzer()

        self.logger.info(f"ğŸš€ Job Monitoring DAG ì‹œì‘ - {self.worksheet_name}")
        df_config = self.sheet_manager.get_all_records_as_df(self.worksheet_name)
        if df_config.empty:
            self.logger.error(f"Google Sheetsì—ì„œ ì„¤ì • ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {self.worksheet_name}")
            return

        # í‚¤ì›Œë“œ í•„í„°ë§ì´ í•„ìš”í•œ ì‹œíŠ¸ ëª©ë¡
        keyword_sheets = ['5000ëŒ€_ê¸°ì—…', '[ë“±ë¡]ì±„ìš©í™ˆí˜ì´ì§€ ëª¨ìŒ']
        if self.worksheet_name in keyword_sheets:
            # ì™¸êµ­ì¸ ì±„ìš©ê³µê³  í‚¤ì›Œë“œ ë¡œë“œ
            self.foreign_keywords = self._load_foreign_keywords()

        if self.worksheet_name == '5000ëŒ€_ê¸°ì—…':
            df_to_process = df_config[df_config['job_posting_url'].notna() & (df_config['job_posting_url'].str.strip() != '')].copy()

            chunk_size = 100
            num_chunks = (len(df_to_process) - 1) // chunk_size + 1
            self.logger.info(f"'{self.worksheet_name}' ì‹œíŠ¸ì˜ {len(df_to_process)}ê°œ ê¸°ì—…ì„ {num_chunks}ê°œ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

            all_current_jobs = {}
            all_warnings = []
            all_failed_companies = []
            list_of_df_chunks = [df_to_process.iloc[i:i+chunk_size] for i in range(0, len(df_to_process), chunk_size)]

            for i, df_chunk in enumerate(list_of_df_chunks):
                start_num = i * chunk_size + 1
                end_num = start_num + len(df_chunk) - 1
                chunk_info = f"{start_num}-{end_num}ë²ˆì§¸ ê¸°ì—…"
                self.logger.info(f"--- ì²­í¬ ì²˜ë¦¬ ì‹œì‘: {chunk_info} ({len(df_chunk)}ê°œ ê¸°ì—…) ---")

                # ê° ì²­í¬ë³„ë¡œ í†µí•© ì²˜ë¦¬ (ì „ì²˜ë¦¬ + í¬ë¡¤ë§)
                self.logger.info(f"ì²­í¬ {i+1}/{num_chunks} í†µí•© ì²˜ë¦¬ ì‹œì‘")
                df_chunk_processed, current_jobs_chunk, failed_companies_chunk = self.process_companies_integrated(df_chunk)

                # ì „ì²´ DataFrameì— ì—…ë°ì´íŠ¸
                df_config.update(df_chunk_processed)
                all_current_jobs.update(current_jobs_chunk)

                # 100ê°œ ì²­í¬ë§ˆë‹¤ ì•ˆì „í•œ ì¤‘ê°„ ì €ì¥
                self.logger.info(f"ì²­í¬ {i+1}/{num_chunks} Google Sheets ì•ˆì „ ì—…ë°ì´íŠ¸ ì¤‘...")
                try:
                    # í—¤ë”ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë°ì´í„°ë§Œ ì—…ë°ì´íŠ¸
                    self.sheet_manager.safe_update_rows(df_config, self.worksheet_name)
                    self.logger.info(f"âœ… ì²­í¬ {i+1}/{num_chunks} ì‹œíŠ¸ ì•ˆì „ ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì´ {len(df_config)} íšŒì‚¬)")
                except Exception as e:
                    self.logger.error(f"âŒ ì²­í¬ {i+1} ì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

                warnings, failed_companies = self.compare_and_notify(current_jobs_chunk, failed_companies_chunk, chunk_info=chunk_info, save=False, send_notifications=False)
                all_warnings.extend(warnings)
                all_failed_companies.extend(failed_companies)

                self.logger.info(f"--- ì²­í¬ ì²˜ë¦¬ ì¢…ë£Œ: {chunk_info} ---")
                if i < num_chunks - 1:
                    self.logger.info(f"ë‹¤ìŒ ì²­í¬ ì²˜ë¦¬ë¥¼ ìœ„í•´ 2ë¶„ê°„ ëŒ€ê¸°í•©ë‹ˆë‹¤.")
                    time.sleep(120)

            if all_warnings or all_failed_companies:
                self.send_slack_notification({}, all_warnings, all_failed_companies, chunk_info="ìš”ì•½")

            if all_current_jobs:
                self.save_jobs(all_current_jobs)

            self.logger.info("ëª¨ë“  ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ. Google Sheetsì— ë³€ê²½ ì‚¬í•­ ì—…ë°ì´íŠ¸ ì¤‘...")
            self.sheet_manager.update_sheet_from_df(df_config, self.worksheet_name)
            self.logger.info("âœ… Google Sheets ì—…ë°ì´íŠ¸ ì™„ë£Œ")

        else:
            original_df_config = df_config.copy()

            self.logger.info("--- í†µí•© ì²˜ë¦¬ ì‹œì‘ (ì „ì²˜ë¦¬ + í¬ë¡¤ë§) ---")
            df_processed, current_jobs, failed_companies = self.process_companies_integrated(df_config)

            updated_count = 0
            for idx in df_processed.index:
                if idx in df_config.index:
                    if pd.isna(original_df_config.loc[idx, 'selenium_required']) or original_df_config.loc[idx, 'selenium_required'] == '':
                        if df_processed.loc[idx, 'selenium_required'] in [0, 1, -1]:
                            updated_count += 1
                    df_config.loc[idx] = df_processed.loc[idx]

            if updated_count > 0:
                self.logger.info(f"ğŸ“ {updated_count}ê°œ íšŒì‚¬ì˜ selenium_required ê°’ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

            has_changes = not df_config.equals(original_df_config)

            if has_changes:
                if len(df_config) < len(original_df_config):
                    self.logger.warning(f"âš ï¸ ë°ì´í„° ì†ì‹¤ ë°©ì§€: ì›ë³¸({len(original_df_config)}ê°œ) ëŒ€ë¹„ í˜„ì¬({len(df_config)}ê°œ)ë¡œ í–‰ì´ ì¤„ì–´ë“¤ì—ˆìŠµë‹ˆë‹¤. ì‹œíŠ¸ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                else:
                    self.logger.info("Google Sheetsì— ë³€ê²½ ì‚¬í•­ ì—…ë°ì´íŠ¸ ì¤‘...")
                    self.sheet_manager.update_sheet_from_df(df_config, self.worksheet_name)
                    self.logger.info("âœ… Google Sheets ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            else:
                self.logger.info("ì„¤ì • ë³€ê²½ ì‚¬í•­ì´ ì—†ì–´ Google Sheets ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

            self.compare_and_notify(current_jobs, failed_companies)

        self.logger.info(f"âœ… Job Monitoring DAG ì¢…ë£Œ - {self.worksheet_name}")

    def _load_foreign_keywords(self):
        """ì™¸êµ­ì¸_ê³µê³ _í‚¤ì›Œë“œ ì‹œíŠ¸ì—ì„œ í‚¤ì›Œë“œë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            df_keywords = self.sheet_manager.get_all_records_as_df('ì™¸êµ­ì¸_ê³µê³ _í‚¤ì›Œë“œ')
            if df_keywords.empty:
                self.logger.info("ì™¸êµ­ì¸ í‚¤ì›Œë“œ ì‹œíŠ¸ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []

            keywords = []
            # Bì—´ë¶€í„° ëª¨ë“  ì—´ì˜ ê°’ë“¤ì„ ìˆ˜ì§‘
            for col in df_keywords.columns[1:]:  # Aì—´(ì¸ë±ìŠ¤) ì œì™¸
                col_keywords = df_keywords[col].dropna().tolist()
                keywords.extend([str(k).strip() for k in col_keywords if str(k).strip()])

            # ì¤‘ë³µ ì œê±° ë° ë¹ˆ ê°’ ì œê±°
            keywords = list(set([k for k in keywords if k and k != 'nan']))
            self.logger.info(f"ì™¸êµ­ì¸ ì±„ìš© í‚¤ì›Œë“œ {len(keywords)}ê°œ ë¡œë“œ ì™„ë£Œ: {keywords[:5]}..." if len(keywords) > 5 else f"ì™¸êµ­ì¸ ì±„ìš© í‚¤ì›Œë“œ ë¡œë“œ: {keywords}")
            return keywords

        except Exception as e:
            self.logger.error(f"ì™¸êµ­ì¸ í‚¤ì›Œë“œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def _is_foreign_job_posting(self, job_title: str) -> bool:
        """ì±„ìš©ê³µê³  ì œëª©ì— ì™¸êµ­ì¸ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        if not self.foreign_keywords:
            return False

        job_title_lower = job_title.lower()
        for keyword in self.foreign_keywords:
            if keyword.lower() in job_title_lower:
                return True
        return False

    def _highlight_foreign_keywords(self, job_title: str) -> Tuple[str, bool]:
        """ì±„ìš©ê³µê³  ì œëª©ì—ì„œ ì™¸êµ­ì¸ í‚¤ì›Œë“œë¥¼ ë³¼ë“œì²˜ë¦¬í•˜ê³ , ì™¸êµ­ì¸ ê³µê³ ì¸ì§€ ì—¬ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not self.foreign_keywords:
            return job_title, False

        is_foreign = False

        # 1ë‹¨ê³„: ì´ë¯¸ *ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë¶€ë¶„ ì°¾ê¸° (ì´ ë¶€ë¶„ì€ ë³´í˜¸)
        markdown_ranges = []
        for match in re.finditer(r'\*[^*]+\*', job_title):
            markdown_ranges.append((match.start(), match.end()))

        # 2ë‹¨ê³„: í‚¤ì›Œë“œ ì°¾ê¸° (ë³´í˜¸ëœ ì˜ì—­ ì œì™¸)
        matches = []
        for keyword in self.foreign_keywords:
            try:
                for match in re.finditer(re.escape(keyword), job_title, re.IGNORECASE):
                    start, end = match.start(), match.end()

                    # ì´ë¯¸ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì²˜ë¦¬ëœ ì˜ì—­ê³¼ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
                    overlap = False
                    for md_start, md_end in markdown_ranges:
                        if not (end <= md_start or start >= md_end):
                            overlap = True
                            break

                    if not overlap:
                        matches.append((start, end))
                        is_foreign = True
            except re.error as e:
                self.logger.warning(f"ì •ê·œì‹ ì˜¤ë¥˜: í‚¤ì›Œë“œ '{keyword}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {e}")
                continue

        if not is_foreign:
            return job_title, False

        # 3ë‹¨ê³„: ë§¤ì¹­ëœ ìœ„ì¹˜ë“¤ì„ ë³‘í•©í•˜ì—¬ ì¤‘ì²© ì œê±°
        if not matches:
            return job_title, is_foreign

        matches.sort()
        merged = [matches[0]]
        for current_start, current_end in matches[1:]:
            last_start, last_end = merged[-1]
            if current_start <= last_end:
                # ì¤‘ì²©ë˜ê±°ë‚˜ ì—°ì†ëœ ê²½ìš°, ë³‘í•©
                merged[-1] = (last_start, max(last_end, current_end))
            else:
                merged.append((current_start, current_end))

        # 4ë‹¨ê³„: ë³¼ë“œ ì²˜ë¦¬ëœ ìƒˆë¡œìš´ ë¬¸ìì—´ ìƒì„±
        highlighted_title = ""
        last_index = 0
        for start, end in merged:
            highlighted_title += job_title[last_index:start]
            highlighted_title += f"*{job_title[start:end]}*"
            last_index = end
        highlighted_title += job_title[last_index:]

        return highlighted_title, is_foreign

    def _process_company_complete(self, args):
        """ì„ íƒì ì°¾ê¸°ì™€ ê³µê³  ìˆ˜ì§‘ì„ í•œë²ˆì— ì²˜ë¦¬"""
        index, row, existing_selectors = args
        company_name = row['íšŒì‚¬_í•œê¸€_ì´ë¦„']
        url = row['job_posting_url']
        selector = row.get('selector', '')
        use_selenium = row['selenium_required']

        self.logger.info(f"- {company_name} ì²˜ë¦¬ ì¤‘...")
        self.company_urls[company_name] = url

        html_content = self.get_html_content_for_crawling(url, use_selenium)

        if not html_content:
            self.logger.error(f"  - HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {company_name} (selenium_requiredë¥¼ -1ë¡œ ì„¤ì •)")
            return index, None, None, {'company': company_name, 'reason': 'HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨', 'url': url, 'selenium_status': -1}

        try:
            soup = BeautifulSoup(html_content, 'html.parser')

            # ì„ íƒìê°€ ì—†ê±°ë‚˜ ë¹ˆ ê²½ìš° ìƒˆë¡œ ì°¾ê¸°
            if not selector or selector.strip() == '':
                self.logger.info(f"  - {company_name} ì„ íƒì ì°¾ê¸° ì¤‘...")
                found_selector = self._try_existing_selectors(soup, existing_selectors, company_name)

                if found_selector:
                    selector = found_selector
                    self.logger.info(f"  - ê¸°ì¡´ ì„ íƒì ì ìš© ì„±ê³µ: {selector}")
                else:
                    best_selector, _ = self.selector_analyzer.find_best_selector(soup)
                    if best_selector:
                        selector = best_selector
                        self.logger.info(f"  - ìƒˆ ì„ íƒì ì°¾ê¸° ì„±ê³µ: {selector}")
                    else:
                        self.logger.warning(f"  - {company_name} ì„ íƒì ì°¾ê¸° ì‹¤íŒ¨ (selenium_requiredë¥¼ -2ë¡œ ì„¤ì •)")
                        return index, None, None, {'company': company_name, 'reason': 'ì„ íƒìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ', 'url': url, 'selenium_status': -2}
            else:
                self.logger.info(f"  - ê¸°ì¡´ ì„ íƒì ì‚¬ìš©: {selector}")

            # ê°™ì€ HTMLë¡œ ê³µê³  ìˆ˜ì§‘
            postings = soup.select(selector)
            if not postings:
                return index, selector, None, {'company': company_name, 'reason': f'ì„ íƒì \'{selector}\'ë¡œ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í•¨', 'url': url}

            # ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ í•„í„°ë§
            all_texts = [post.get_text(strip=True) for post in postings if post.get_text(strip=True).strip()]
            # ì±„ìš©ê³µê³ ê°€ ë§ëŠ” ê²ƒë“¤ë§Œ í•„í„°ë§
            job_titles = {text for text in all_texts if self.selector_analyzer._is_potential_job_posting(text)}

            if job_titles:
                self.logger.info(f"  - ì„±ê³µ: ì„ íƒì ì ìš© + {len(job_titles)}ê°œ ê³µê³  ìˆ˜ì§‘")
                return index, selector, job_titles, None
            else:
                return index, selector, None, {'company': company_name, 'reason': 'ìœ íš¨í•œ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í•¨', 'url': url}

        except Exception as e:
            self.logger.error(f"  - {company_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return index, None, None, {'company': company_name, 'reason': f'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}', 'url': url}

    def process_companies_integrated(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict, List]:
        """ì „ì²˜ë¦¬ì™€ í¬ë¡¤ë§ì„ í•œë²ˆì— í†µí•© ì²˜ë¦¬"""
        self.logger.info(f"í†µí•© ì²˜ë¦¬ ëŒ€ìƒ: {len(df)}ê°œ íšŒì‚¬")

        # 1. selenium_required ê°’ ì±„ìš°ê¸°
        valid_companies_mask = (
            df['íšŒì‚¬_í•œê¸€_ì´ë¦„'].notna() & (df['íšŒì‚¬_í•œê¸€_ì´ë¦„'].str.strip() != '') &
            df['job_posting_url'].notna() & (df['job_posting_url'].str.strip() != '')
        )

        if valid_companies_mask.any():
            self._fill_missing_selenium_required(df, valid_companies_mask)

        # 2. ì²˜ë¦¬ ê°€ëŠ¥í•œ íšŒì‚¬ë“¤ í•„í„°ë§ (HTML ì‹¤íŒ¨(-1), ì„ íƒì ì‹¤íŒ¨(-2) ì œì™¸)
        companies_to_process = df[
            valid_companies_mask &
            (~df['selenium_required'].isin([-1, -2]))
        ]

        if companies_to_process.empty:
            self.logger.info("ì²˜ë¦¬í•  íšŒì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return df, {}, []

        # 3. ê¸°ì¡´ ì„ íƒì ìˆ˜ì§‘
        existing_selectors = self._get_existing_selectors(df)
        self.logger.info(f"ê¸°ì¡´ ì„ íƒì {len(existing_selectors)}ê°œ (20ì ì´ìƒë§Œ) í™œìš©")

        # 4. í†µí•© ì²˜ë¦¬ (ì„ íƒì ì°¾ê¸° + í¬ë¡¤ë§)
        current_jobs = {}
        failed_companies = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            args_list = [(index, row, existing_selectors) for index, row in companies_to_process.iterrows()]
            results = executor.map(self._process_company_complete, args_list)

            for index, selector, job_titles, error_info in results:
                if selector:
                    df.loc[index, 'selector'] = selector

                if job_titles:
                    company_name = companies_to_process.loc[index, 'íšŒì‚¬_í•œê¸€_ì´ë¦„']
                    current_jobs[company_name] = job_titles
                elif error_info:
                    # ì‹¤íŒ¨ ìœ í˜•ë³„ë¡œ selenium_required ê°’ ì„¤ì •
                    if 'selenium_status' in error_info:
                        df.loc[index, 'selenium_required'] = error_info['selenium_status']
                        self.logger.info(f"  - {error_info['company']} selenium_requiredë¥¼ {error_info['selenium_status']}ë¡œ ì„¤ì •")
                    failed_companies.append(error_info)

        # 5. ì„ íƒì ì•ˆì •í™”
        df = self.stabilize_selectors(df)

        self.logger.info(f"í†µí•© ì²˜ë¦¬ ì™„ë£Œ: ì„±ê³µ {len(current_jobs)}ê°œ, ì‹¤íŒ¨ {len(failed_companies)}ê°œ")
        return df, current_jobs, failed_companies

    def preprocess_companies(self, df: pd.DataFrame) -> pd.DataFrame:
        self.logger.info(f"ì „ì²´ íšŒì‚¬ ë°ì´í„°: {len(df)}ê°œ")

        valid_companies_mask = (
            df['íšŒì‚¬_í•œê¸€_ì´ë¦„'].notna() & (df['íšŒì‚¬_í•œê¸€_ì´ë¦„'].str.strip() != '') &
            df['job_posting_url'].notna() & (df['job_posting_url'].str.strip() != '')
        )

        invalid_count = len(df) - valid_companies_mask.sum()
        if invalid_count > 0:
            self.logger.info(f"URLì´ ì—†ì–´ ì „ì²˜ë¦¬ì—ì„œ ì œì™¸ë˜ëŠ” íšŒì‚¬: {invalid_count}ê°œ (ì‹œíŠ¸ì—ì„œëŠ” ìœ ì§€ë¨)")

        self.logger.info(f"ì „ì²˜ë¦¬ ëŒ€ìƒ íšŒì‚¬ (URL í¬í•¨): {valid_companies_mask.sum()}ê°œ")

        if valid_companies_mask.any():
            self._fill_missing_selenium_required(df, valid_companies_mask)

        companies_to_process = df[
            valid_companies_mask &
            (df['selector'].isna() | (df['selector'] == '')) &
            (df['original_selector'].isna() | (df['original_selector'] == '')) &
            (~df['selenium_required'].isin([-1, -2]))
        ]

        if companies_to_process.empty:
            self.logger.info("ìƒˆë¡œ ì „ì²˜ë¦¬í•  íšŒì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return df

        self.logger.info(f"{len(companies_to_process)}ê°œ íšŒì‚¬ì— ëŒ€í•œ ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

        existing_selectors = self._get_existing_selectors(df)
        self.logger.info(f"ê¸°ì¡´ íšŒì‚¬ë“¤ì—ì„œ ì‚¬ìš© ì¤‘ì¸ ì„ íƒì {len(existing_selectors)}ê°œ (20ì ì´ìƒë§Œ)ë¥¼ ìš°ì„  ì ìš©í•©ë‹ˆë‹¤.")

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            args_list = [(index, row, existing_selectors) for index, row in companies_to_process.iterrows()]
            results = executor.map(self._process_company_preprocess, args_list)

            for index, new_selector, selenium_status in results:
                if new_selector:
                    df.loc[index, 'selector'] = new_selector
                    self.logger.info(f"  - ì„ íƒì ì ìš© ì„±ê³µ: {new_selector}")
                if selenium_status is not None:
                    df.loc[index, 'selenium_required'] = selenium_status

        return df

    def stabilize_selectors(self, df: pd.DataFrame) -> pd.DataFrame:
        changed = False
        for index, row in df.iterrows():
            selector = row['selector']
            original_selector = row.get('original_selector', '')

            if (pd.isna(selector) or selector == '') and pd.notna(original_selector) and original_selector != '':
                stabilized = stabilize_selector(original_selector, conservative=False)
                df.loc[index, 'selector'] = stabilized
                self.logger.info(f"- {row['íšŒì‚¬_í•œê¸€_ì´ë¦„']} original_selectorë¥¼ ì•ˆì •í™”í•˜ì—¬ ì ìš©: {original_selector} -> {stabilized}")
                changed = True
            elif pd.notna(selector) and selector != '':
                stabilized = stabilize_selector(selector, conservative=True)
                if selector != stabilized:
                    df.loc[index, 'selector'] = stabilized
                    self.logger.info(f"- {row['íšŒì‚¬_í•œê¸€_ì´ë¦„']} ì„ íƒì ì•ˆì •í™”: {selector} -> {stabilized}")
                    changed = True
        if not changed:
            self.logger.info("ì•ˆì •í™”í•  ì„ íƒìê°€ ì—†ìŠµë‹ˆë‹¤.")
        return df


    def _get_existing_selectors(self, df: pd.DataFrame) -> List[str]:
        """ê¸°ì¡´ì— ì„±ê³µì ìœ¼ë¡œ ì‚¬ìš©ëœ ì„ íƒìë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤ (20ì ì´ìƒë§Œ)."""
        existing_selectors = []

        # 20ì ì´ìƒì˜ ì„ íƒìë§Œ ìˆ˜ì§‘
        valid_selectors = df[df['selector'].notna() & (df['selector'] != '')]['selector'].unique()
        valid_selectors = [s for s in valid_selectors if len(s) >= 20]
        existing_selectors.extend(valid_selectors)

        expanded_selectors = []
        for selector in valid_selectors:
            expanded_selectors.append(selector)
            stabilized = stabilize_selector(selector, conservative=False)
            if stabilized != selector and stabilized and len(stabilized) >= 20:
                expanded_selectors.append(stabilized)

        # known_good_selectorsë„ 20ì ì´ìƒë§Œ í¬í•¨
        known_good_selectors = [
            "a div.sc-9b56f69e-0.jlntFl",
            "div.JobPostingsJobPosting__Layout-sc-6ae888f2-0.ffnSOB div.JobPostingsJobPosting__Bottom-sc-6ae888f2-5.iXrIoX",
            "#jobList > div.jobList_info > div > a > span.title",
            "div.RecruitList_left__5MzDR div.RecruitList_title-wrapper__Gvh1r p",
            "div.swiper-slide button p",
        ]
        # 20ì ì´ìƒì¸ ê²ƒë§Œ ì¶”ê°€
        known_good_selectors = [s for s in known_good_selectors if len(s) >= 20]
        expanded_selectors.extend(known_good_selectors)

        selector_counts = df[df['selector'].notna() & (df['selector'] != '')]['selector'].value_counts()
        sorted_selectors = [s for s in selector_counts.index.tolist() if len(s) >= 20]

        final_selectors = sorted_selectors + [s for s in expanded_selectors if s not in sorted_selectors]

        self.logger.info(f"ìˆ˜ì§‘ëœ ì„ íƒì {len(final_selectors)}ê°œ (20ì ì´ìƒë§Œ, ê¸°ì¡´: {len(sorted_selectors)}ê°œ, í™•ì¥: {len(expanded_selectors)}ê°œ)")
        return final_selectors

    def _is_specific_enough_selector(self, selector: str) -> bool:
        """ì„ íƒìê°€ ì¶©ë¶„íˆ êµ¬ì²´ì ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
        selector = selector.strip()
        if not selector:
            return False

        parts = selector.split()

        if len(parts) == 1:
            part = parts[0].lower()
            if '.' in part or '#' in part:
                return True
            if '[' in part and ']' in part:
                if part == 'a[href]' or part.endswith('[href]') :
                    return False
                return True
            if ':' in part:
                return True
            return False

        if len(parts) >= 2:
            return True

        return False

    def _try_existing_selectors(self, soup: BeautifulSoup, existing_selectors: List[str], _: str) -> Optional[str]:
        """ê¸°ì¡´ ì„ íƒìë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹œë„í•´ì„œ ìœ íš¨í•œ ê²ƒì„ ì°¾ìŠµë‹ˆë‹¤."""
        # 20ì ì´ìƒì˜ ì„ íƒìë§Œ ì¬í™œìš© ì‹œë„
        valid_selectors = [s for s in existing_selectors if len(s) >= 20 and self._is_specific_enough_selector(s)]

        for i, selector in enumerate(valid_selectors):
            # ì´ë¯¸ í•„í„°ë§ë˜ì—ˆìœ¼ë¯€ë¡œ ê¸¸ì´ ì²´í¬ ë¶ˆí•„ìš”

            try:
                elements = soup.select(selector)
                if not elements:
                    continue

                titles = [elem.get_text(strip=True) for elem in elements]
                valid_titles = [title for title in titles if title and len(title) > 3]

                if len(valid_titles) >= 1:
                    job_related_titles = [title for title in valid_titles
                                        if self.selector_analyzer._is_potential_job_posting(title)]

                    if len(job_related_titles) >= 1:
                        quality_score = len(job_related_titles) / len(valid_titles)

                        if len(valid_titles) > 50:
                            if quality_score < 0.8:
                                continue
                        elif len(valid_titles) > 20:
                            if quality_score < 0.6:
                                continue
                        elif len(valid_titles) > 5:
                            if quality_score < 0.4:
                                continue

                        category = "ê¸°ì¡´" if i < 10 else "í™•ì¥" if i < 50 else "íŒ¨í„´"
                        self.logger.info(f"  - {category} ì„ íƒì '{selector}' ê²€ì¦ ì„±ê³µ (ì±„ìš©ê³µê³ : {len(job_related_titles)}ê°œ/{len(valid_titles)}ê°œ, í’ˆì§ˆ: {quality_score:.1%})")
                        if len(valid_titles) <= 5:
                            for title in valid_titles[:3]:
                                self.logger.info(f"    ì˜ˆì‹œ: {title[:50]}...")
                        return selector

            except Exception:
                continue

        return None

    def _fill_missing_selenium_required(self, df: pd.DataFrame, mask: pd.Series):
        """selenium_required ê°’ì´ ì—†ëŠ” íšŒì‚¬ë“¤ì„ ìë™ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤. (ë³‘ë ¬ ì²˜ë¦¬)"""
        missing_selenium_mask = mask & (
            df['selenium_required'].isna() |
            (df['selenium_required'] == '') |
            (~df['selenium_required'].isin([0, 1, -1]))
        )
        missing_selenium = df[missing_selenium_mask]

        if missing_selenium.empty:
            self.logger.info("ëª¨ë“  íšŒì‚¬ì˜ selenium_required ê°’ì´ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return

        self.logger.info(f"{len(missing_selenium)}ê°œ íšŒì‚¬ì˜ selenium_required ê°’ì„ ë³‘ë ¬ë¡œ ìë™ ì„¤ì • ì¤‘...")

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_index = {
                executor.submit(self._determine_selenium_requirement, row['job_posting_url'], row['íšŒì‚¬_í•œê¸€_ì´ë¦„']): index
                for index, row in missing_selenium.iterrows()
            }

            for future in concurrent.futures.as_completed(future_to_index):
                index = future_to_index[future]
                company_name = missing_selenium.loc[index, 'íšŒì‚¬_í•œê¸€_ì´ë¦„']
                try:
                    selenium_required = future.result()
                    df.loc[index, 'selenium_required'] = int(selenium_required)

                    selenium_text = "Selenium í•„ìš”" if selenium_required else "requests ì‚¬ìš©"
                    self.logger.info(f"  - {company_name}: {selenium_text}")
                except Exception as e:
                    self.logger.error(f"  - {company_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    df.loc[index, 'selenium_required'] = 1  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’

        self.logger.info(f"{len(missing_selenium)}ê°œ íšŒì‚¬ì˜ selenium_required ê°’ ì„¤ì • ì™„ë£Œ.")
    
    def _determine_selenium_requirement(self, url: str, _: str) -> int:
        """URLì„ ê¸°ë°˜ìœ¼ë¡œ Selenium í•„ìš” ì—¬ë¶€ë¥¼ ë™ì ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤."""

        try:
            selenium_req = self.selenium_checker.check_selenium_requirement(url)
            result = int(selenium_req)
            self.logger.info(f"    ğŸ” Selenium ì²´í¬ ê²°ê³¼: {url} -> {result}")
            return result
        except Exception as e:
            self.logger.info(f"    âš ï¸ Selenium ì²´í¬ ì‹¤íŒ¨ ({url}): {e} -> ê¸°ë³¸ê°’ 1 ì‚¬ìš©")
            return 1

    def _crawl_company(self, args):
        row, _ = args
        company_name, url, use_selenium, selector = row['íšŒì‚¬_í•œê¸€_ì´ë¦„'], row['job_posting_url'], row['selenium_required'], row['selector']
        self.company_urls[company_name] = url
        self.logger.info(f"- {company_name} í¬ë¡¤ë§ ì¤‘...")

        html_content = self.get_html_content_for_crawling(url, use_selenium, selector)

        if not html_content:
            return None, {'company': company_name, 'reason': 'HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨', 'url': url}

        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            postings = soup.select(selector)
            if not postings:
                return None, {'company': company_name, 'reason': f'ì„ íƒì \'{selector}\'ë¡œ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í•¨', 'url': url}

            job_titles = {post.get_text(strip=True) for post in postings if post.get_text(strip=True).strip()}
            if job_titles:
                self.logger.info(f"  - ì„±ê³µ: {len(job_titles)}ê°œ ê³µê³  ì¶”ì¶œ")
                return company_name, job_titles
            else:
                return None, {'company': company_name, 'reason': 'ìœ íš¨í•œ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í•¨', 'url': url}
        except Exception as e:
            return None, {'company': company_name, 'reason': f'HTML íŒŒì‹± ì‹¤íŒ¨: {e}', 'url': url}

    def crawl_jobs(self, df_config: pd.DataFrame) -> Tuple[Dict[str, Set[str]], List[Dict]]:
        self.logger.info("--- 3. ì±„ìš© ê³µê³  í¬ë¡¤ë§ ì‹œì‘ ---")
        df_crawl = df_config[
            (df_config['íšŒì‚¬_í•œê¸€_ì´ë¦„'].notna() & (df_config['íšŒì‚¬_í•œê¸€_ì´ë¦„'].str.strip() != '')) &
            (df_config['job_posting_url'].notna() & (df_config['job_posting_url'].str.strip() != '')) &
            (df_config['selector'].notna() & (df_config['selector'] != '')) &
            (~df_config['selenium_required'].isin([-1, -2]))
        ].copy()

        if df_crawl.empty:
            self.logger.warning("í¬ë¡¤ë§í•  íšŒì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}, []

        current_jobs = {}
        failed_companies = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            args_list = [(row, df_crawl) for _, row in df_crawl.iterrows()]
            results = executor.map(self._crawl_company, args_list)

            for result in results:
                company_name, job_titles = result
                if company_name and job_titles:
                    current_jobs[company_name] = job_titles
                elif job_titles:
                    failed_companies.append(job_titles)

        self.logger.info("--- 3. ì±„ìš© ê³µê³  í¬ë¡¤ë§ ì¢…ë£Œ ---")
        return current_jobs, failed_companies

    def compare_and_notify(self, current_jobs: Dict, failed_companies: List, chunk_info: str = None, save: bool = True, send_notifications: bool = True) -> Tuple[List, List]:
        self.logger.info("--- 4. ë¹„êµ ë° ì•Œë¦¼ ì‹œì‘ ---")
        existing_jobs = self.load_existing_jobs()
        new_jobs = self.find_new_jobs(current_jobs, existing_jobs)
        warnings = self.check_suspicious_results(current_jobs, existing_jobs, new_jobs)

        # send_notificationsê°€ Trueì¼ ë•Œë§Œ ìƒˆë¡œìš´ ê³µê³  ì¦‰ì‹œ ì•Œë¦¼
        if send_notifications and new_jobs:
            self.send_slack_notification(new_jobs, [], [], chunk_info=chunk_info)

        if save and current_jobs:
            self.save_jobs(current_jobs)
        self.logger.info("--- 4. ë¹„êµ ë° ì•Œë¦¼ ì¢…ë£Œ ---")
        return warnings, failed_companies

    def get_html_content(self, url, use_selenium, selector=None):
        """ì„ íƒì ë¶„ì„ìš© HTML ê°€ì ¸ì˜¤ê¸° ë©”ì„œë“œ (Playwright ì‚¬ìš©)"""
        max_retries = 2
        for attempt in range(max_retries):
            try:
                if not use_selenium:
                    # ì„¸ì…˜ì— ì´ë¯¸ í—¤ë”ê°€ ì„¤ì •ë˜ì–´ ìˆìŒ
                    response = self.session.get(url, timeout=20)
                    response.raise_for_status()
                    return response.text
                else:
                    playwright, browser = self.create_playwright_browser()
                    if not playwright or not browser:
                        raise Exception("Playwright ë¸Œë¼ìš°ì €ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                    try:
                        page = browser.new_page()
                        page.goto(url, timeout=20000)

                        if selector:
                            try:
                                page.wait_for_selector(selector, timeout=20000)
                                time.sleep(3)
                            except Exception:
                                self.logger.warning(f"'{selector}' ìš”ì†Œë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        else:
                            time.sleep(5)

                        html_content = page.content()
                        return html_content
                    finally:
                        browser.close()
                        playwright.stop()

            except Exception as e:
                if "timeout" in str(e).lower() and attempt < max_retries - 1:
                    self.logger.warning(f"í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ ({attempt + 1}/{max_retries}): {url} - ì¬ì‹œë„ ì¤‘...")
                    time.sleep(5)
                    continue
                else:
                    self.logger.error(f"HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}")
                    return None

    def get_html_content_for_crawling(self, url, use_selenium, selector=None):
        """ì‹¤ì œ í¬ë¡¤ë§ìš© HTML ê°€ì ¸ì˜¤ê¸° ë©”ì„œë“œ (Playwright ì‚¬ìš©)"""
        try:
            if not use_selenium:
                # ë” í˜„ì‹¤ì ì¸ ë¸Œë¼ìš°ì € í—¤ë” ì‚¬ìš©
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'DNT': '1',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'none',
                    'Sec-Fetch-User': '?1',
                    'Cache-Control': 'max-age=0',
                    'Referer': url  # ë¦¬í¼ëŸ¬ ì¶”ê°€ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë¸Œë¼ìš°ì§• ì‹œë®¬ë ˆì´ì…˜
                }
                response = requests.get(url, headers=headers, timeout=20, verify=False)
                response.raise_for_status()
                self.logger.debug(f"HTTP ìš”ì²­ ì„±ê³µ: {url} (ì‘ë‹µ ì½”ë“œ: {response.status_code})")
                return response.text
            else:
                playwright, browser = self.create_playwright_browser()
                if not playwright or not browser:
                    raise Exception("Playwright ë¸Œë¼ìš°ì €ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                max_retries = 2
                for attempt in range(max_retries):
                    try:
                        page = browser.new_page()
                        page.goto(url, timeout=20000)

                        if selector:
                            try:
                                page.wait_for_selector(selector, timeout=20000)
                                time.sleep(3)
                            except Exception:
                                self.logger.warning(f"ì„ íƒì '{selector}' ìš”ì†Œë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        else:
                            time.sleep(5)

                        html_content = page.content()
                        browser.close()
                        playwright.stop()
                        return html_content

                    except Exception as e:
                        if "timeout" in str(e).lower() and attempt < max_retries - 1:
                            self.logger.warning(f"í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ ({attempt + 1}/{max_retries}): {url} - ì¬ì‹œë„ ì¤‘...")
                            time.sleep(5)
                            continue
                        else:
                            browser.close()
                            playwright.stop()
                            raise e

        except requests.exceptions.Timeout as e:
            self.logger.error(f"í¬ë¡¤ë§ìš© HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (íƒ€ì„ì•„ì›ƒ): {url} - {str(e)}")
            return None
        except requests.exceptions.ConnectionError as e:
            self.logger.error(f"í¬ë¡¤ë§ìš© HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ì—°ê²° ì˜¤ë¥˜): {url} - {str(e)}")
            return None
        except requests.exceptions.HTTPError as e:
            self.logger.error(f"í¬ë¡¤ë§ìš© HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (HTTP {e.response.status_code}): {url} - {str(e)}")
            return None
        except requests.exceptions.SSLError as e:
            self.logger.error(f"í¬ë¡¤ë§ìš© HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (SSL ì˜¤ë¥˜): {url} - {str(e)}")
            return None
        except Exception as e:
            self.logger.error(f"í¬ë¡¤ë§ìš© HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ê¸°íƒ€ ì˜¤ë¥˜): {url} - {type(e).__name__}: {str(e)}")
            return None

    def create_playwright_browser(self):
        """Playwright ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        try:
            playwright = sync_playwright().start()
            browser = playwright.chromium.launch(
                headless=True,
                args=[
                    "--no-sandbox",
                    "--disable-dev-shm-usage",
                    "--disable-gpu",
                    "--disable-web-security",
                    "--disable-features=VizDisplayCompositor",
                    "--ignore-certificate-errors",
                    "--ignore-ssl-errors",
                    "--ignore-certificate-errors-spki-list"
                ]
            )
            self.logger.info("Playwright ë¸Œë¼ìš°ì € ì‹¤í–‰ ì„±ê³µ")
            return playwright, browser
        except Exception as e:
            self.logger.error(f"Playwright ë¸Œë¼ìš°ì € ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None, None

    def load_existing_jobs(self) -> Dict[str, Set[str]]:
        if not os.path.exists(self.results_path):
            return {}
        try:
            df = pd.read_csv(self.results_path, encoding='utf-8-sig')
            return {comp: set(df_comp['job_posting_title']) for comp, df_comp in df.groupby('íšŒì‚¬_í•œê¸€_ì´ë¦„')}
        except Exception as e:
            self.logger.error(f"ê¸°ì¡´ ê³µê³  ë¡œë“œ ì˜¤ë¥˜: {e}")
            return {}

    def find_new_jobs(self, current_jobs: Dict, existing_jobs: Dict) -> Dict[str, List[str]]:
        new_jobs = {comp: list(curr - existing_jobs.get(comp, set())) for comp, curr in current_jobs.items()}
        return {c: j for c, j in new_jobs.items() if j}

    def check_suspicious_results(self, current_jobs: Dict, existing_jobs: Dict, new_jobs: Dict) -> List[str]:
        warnings = []
        for company, new_list in new_jobs.items():
            if len(existing_jobs.get(company, set())) > 0 and len(new_list) == len(current_jobs.get(company, set())):
                warnings.append(f"{company}: ê¸°ì¡´ ê³µê³ ê°€ ëª¨ë‘ ì‚¬ë¼ì§€ê³  ìƒˆë¡œìš´ ê³µê³ ë§Œ ë³´ì…ë‹ˆë‹¤. í™ˆí˜ì´ì§€ë¥¼ ì§ì ‘ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return warnings

    def save_jobs(self, current_jobs: Dict):
        kst = pytz.timezone('Asia/Seoul')
        current_time_kst = datetime.now(kst)
        all_postings = [{'íšŒì‚¬_í•œê¸€_ì´ë¦„': comp, 'job_posting_title': title, 'crawl_datetime': current_time_kst.strftime('%Y-%m-%d %H:%M:%S')} for comp, titles in current_jobs.items() for title in titles]

        try:
            pd.DataFrame(all_postings).to_csv(self.results_path, index=False, encoding='utf-8-sig')
            self.logger.info(f"ê²°ê³¼ë¥¼ '{self.results_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        except PermissionError as e:
            self.logger.error(f"íŒŒì¼ ì €ì¥ ê¶Œí•œ ì˜¤ë¥˜: {e}")
            # ëŒ€ì•ˆ ê²½ë¡œ ì‹œë„
            import tempfile
            temp_path = os.path.join(tempfile.gettempdir(), 'job_postings_latest.csv')
            try:
                pd.DataFrame(all_postings).to_csv(temp_path, index=False, encoding='utf-8-sig')
                self.logger.info(f"ì„ì‹œ ê²½ë¡œì— ê²°ê³¼ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤: '{temp_path}'")
            except Exception as e2:
                self.logger.error(f"ì„ì‹œ íŒŒì¼ ì €ì¥ë„ ì‹¤íŒ¨: {e2}")
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def send_slack_notification(self, new_jobs: Dict, warnings: List, failed_companies: List, chunk_info: str = None):
        if not self.webhook_url:
            self.logger.error(f"{self.webhook_url_env}ì´ .envì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        if not new_jobs and not warnings and not failed_companies:
            self.logger.info("ì•Œë¦¼ ë³´ë‚¼ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        kst = pytz.timezone('Asia/Seoul')
        current_time = datetime.now(kst).strftime('%H:%M')
        current_datetime = datetime.now(kst)
        # ìš”ì¼ í•œê¸€í™”
        weekdays = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
        formatted_datetime = f"{current_datetime.month}ì›” {current_datetime.day}ì¼ ({weekdays[current_datetime.weekday()]}) {current_datetime.strftime('%H:%M')}"
        
        blocks = []

        if new_jobs:
            total_new_jobs = sum(len(jobs) for jobs in new_jobs.values())
            foreign_job_count = sum(1 for jobs in new_jobs.values() for job in jobs if self._is_foreign_job_posting(job))

            chunk_str = f"({chunk_info}) " if chunk_info else ""
            foreign_info = f" (ì™¸êµ­ì¸ ì±„ìš©: {foreign_job_count}ê°œ ğŸ”®)" if foreign_job_count > 0 else ""
            header_text = f"ğŸ‰ *ìƒˆë¡œìš´ ì±„ìš©ê³µê³  {total_new_jobs}ê°œ ë°œê²¬!*{foreign_info} {chunk_str}({current_time})"
            blocks.append({
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": header_text
                }
            })
            blocks.append({"type": "divider"})

            for company, jobs in new_jobs.items():
                company_url = self.company_urls.get(company, "")
                linked_company = f"<{company_url}|{company}>" if company_url else f"*{company}*"
                company_with_time = f"{linked_company} - {formatted_datetime}"
                
                job_lines = []
                for job in jobs:
                    highlighted_job, is_foreign = self._highlight_foreign_keywords(job)
                    job_line = f"â€¢ {highlighted_job}"
                    if is_foreign:
                        job_line = f"ğŸ”® {job_line}"
                    job_lines.append(job_line)
                
                job_text = "\n".join(job_lines)
                
                company_section = {
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": f"ğŸ“¢ {company_with_time} - {len(jobs)}ê°œ\n{job_text}"
                    }
                }
                blocks.append(company_section)

        if warnings:
            warning_text = "âš ï¸ *í™•ì¸ì´ í•„ìš”í•œ ê³µê³ * (í™ˆí˜ì´ì§€ë¥¼ ì§ì ‘ í™•ì¸í•´ì£¼ì„¸ìš”)\n" + "\n".join([f"â€¢ {w}" for w in warnings])
            blocks.append({"type": "divider"})
            blocks.append({
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": warning_text
                }
            })

        if failed_companies:
            fail_text = "âŒ *í¬ë¡¤ë§ ì‹¤íŒ¨*\n" + "\n".join([f"â€¢ {f['company']}: {f['reason']}" for f in failed_companies])
            blocks.append({"type": "divider"})
            blocks.append({
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": fail_text
                }
            })

        payload = {
            "blocks": blocks,
            "username": "ì±„ìš©ê³µê³  ì•Œë¦¬ë¯¸",
            "icon_emoji": ":robot_face:"
        }
        
        try:
            self.logger.info(f"ğŸ“¤ ìŠ¬ë™ ë©”ì‹œì§€ ì „ì†¡ ì‹œë„ (ë¸”ë¡ ê°œìˆ˜: {len(blocks)})")
            response = requests.post(self.webhook_url, json=payload, timeout=15)

            if response.status_code == 200:
                self.logger.info("âœ… ìŠ¬ë™ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
            else:
                self.logger.error(f"âŒ ìŠ¬ë™ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code} - {response.text}")

        except Exception as e:
            self.logger.error(f"âŒ ìŠ¬ë™ ì•Œë¦¼ ì „ì†¡ ì˜¤ë¥˜: {e}")

def main():
    base_dir = os.path.dirname(os.path.abspath(__file__))
    dag = JobMonitoringDAG(base_dir)
    dag.run()

if __name__ == "__main__":
    main()
