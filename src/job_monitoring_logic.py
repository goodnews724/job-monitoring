import os
import time
import pandas as pd
import requests
import re
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime
import pytz
import logging
from typing import Dict, List, Set, Tuple, Optional
from dotenv import load_dotenv
from google_sheet_utils import GoogleSheetManager
from analyze_titles import JobPostingSelectorAnalyzer
from utils import stabilize_selector, SeleniumRequirementChecker
import concurrent.futures

load_dotenv()

class JobMonitoringDAG:
    def __init__(self, base_dir: str, worksheet_name: str = '[ë“±ë¡]ì±„ìš©í™ˆí˜ì´ì§€ ëª¨ìŒ', webhook_url_env: str = 'SLACK_WEBHOOK_URL', results_filename: str = 'job_postings_latest.csv'):
        self.base_dir = base_dir
        self.data_dir = os.path.join(base_dir, 'data')
        self.html_dir = os.path.join(base_dir, 'html')
        # HTML ë””ë ‰í† ë¦¬ ìƒì„± (ë” ì•ˆì „í•œ ë°©ì‹)
        try:
            os.makedirs(self.html_dir, exist_ok=True)
        except (FileExistsError, OSError):
            # ì´ë¯¸ ì¡´ì¬í•˜ê±°ë‚˜ ê¶Œí•œ ë¬¸ì œê°€ ìˆì–´ë„ ê³„ì† ì§„í–‰
            pass
        self.worksheet_name = worksheet_name
        self.webhook_url_env = webhook_url_env  # í™˜ê²½ë³€ìˆ˜ ì´ë¦„ ì €ì¥
        self.results_path = os.path.join(self.data_dir, results_filename)
        self.webhook_url = os.getenv(webhook_url_env)
        self.company_urls = {}

        self._setup_logging()

    def _setup_logging(self):
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)

        # ì´ ë¡œê±°ë‚˜ ë£¨íŠ¸ ë¡œê±°ì— í•¸ë“¤ëŸ¬ê°€ ì—†ì„ ë•Œë§Œ í•¸ë“¤ëŸ¬ë¥¼ ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
        if not self.logger.handlers and not logging.getLogger().handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)

    def run(self):
        self.sheet_manager = GoogleSheetManager(self.base_dir)
        self.selenium_checker = SeleniumRequirementChecker()
        self.selector_analyzer = JobPostingSelectorAnalyzer()

        self.logger.info(f"ğŸš€ Job Monitoring DAG ì‹œì‘ - {self.worksheet_name}")
        df_config = self.sheet_manager.get_all_records_as_df(self.worksheet_name)
        if df_config.empty:
            self.logger.error(f"Google Sheetsì—ì„œ ì„¤ì • ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {self.worksheet_name}")
            return

        original_df_config = df_config.copy()

        self.logger.info("--- 1. ì „ì²˜ë¦¬ ì‹œì‘ ---")
        df_processed = self.preprocess_companies(df_config)
        self.logger.info("--- 1. ì „ì²˜ë¦¬ ì¢…ë£Œ ---")

        self.logger.info("--- 2. ì„ íƒì ì•ˆì •í™” ì‹œì‘ ---")
        df_processed = self.stabilize_selectors(df_processed)
        self.logger.info("--- 2. ì„ íƒì ì•ˆì •í™” ì¢…ë£Œ ---")

        updated_count = 0
        for idx in df_processed.index:
            if idx in df_config.index:
                if pd.isna(original_df_config.loc[idx, 'selenium_required']) or original_df_config.loc[idx, 'selenium_required'] == '':
                    if df_processed.loc[idx, 'selenium_required'] in [0, 1, -1]:
                        updated_count += 1
                df_config.loc[idx] = df_processed.loc[idx]

        if updated_count > 0:
            self.logger.info(f"ğŸ“ {updated_count}ê°œ íšŒì‚¬ì˜ selenium_required ê°’ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

        has_changes = not df_config.equals(original_df_config)

        if has_changes:
            if len(df_config) < len(original_df_config):
                self.logger.warning(f"âš ï¸ ë°ì´í„° ì†ì‹¤ ë°©ì§€: ì›ë³¸({len(original_df_config)}ê°œ) ëŒ€ë¹„ í˜„ì¬({len(df_config)}ê°œ)ë¡œ í–‰ì´ ì¤„ì–´ë“¤ì—ˆìŠµë‹ˆë‹¤. ì‹œíŠ¸ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            else:
                self.logger.info("Google Sheetsì— ë³€ê²½ ì‚¬í•­ ì—…ë°ì´íŠ¸ ì¤‘...")
                self.sheet_manager.update_sheet_from_df(df_config, self.worksheet_name)
                self.logger.info("âœ… Google Sheets ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        else:
            self.logger.info("ì„¤ì • ë³€ê²½ ì‚¬í•­ì´ ì—†ì–´ Google Sheets ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

        current_jobs, failed_companies = self.crawl_jobs(df_processed)
        self.compare_and_notify(current_jobs, failed_companies)
        self.logger.info(f"âœ… Job Monitoring DAG ì¢…ë£Œ - {self.worksheet_name}")

    def _process_company_preprocess(self, args):
        index, row, existing_selectors = args
        company_name = row['íšŒì‚¬_í•œê¸€_ì´ë¦„']
        url = row['job_posting_url']
        self.logger.info(f"- {company_name} ì²˜ë¦¬ ì¤‘...")

        driver = self.create_minimal_driver() if row['selenium_required'] else None
        html_content = self.get_html_content(url, row['selenium_required'], driver)
        if driver:
            driver.quit()

        if html_content:
            self.logger.info(f"  - HTML ê°€ì ¸ì˜¤ê¸° ì„±ê³µ")
            soup = BeautifulSoup(html_content, 'html.parser')
            found_selector = self._try_existing_selectors(soup, existing_selectors, company_name)

            if found_selector:
                return index, found_selector, None
            else:
                best_selector, _ = self.selector_analyzer.find_best_selector(soup)
                if best_selector:
                    return index, best_selector, None
                else:
                    self.logger.warning("  - ì„ íƒì ë¶„ì„ ì‹¤íŒ¨")
                    return index, None, None
        else:
            self.logger.error(f"  - HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {company_name} (selenium_requiredë¥¼ -1ë¡œ ì„¤ì •)")
            return index, None, -1

    def preprocess_companies(self, df: pd.DataFrame) -> pd.DataFrame:
        self.logger.info(f"ì „ì²´ íšŒì‚¬ ë°ì´í„°: {len(df)}ê°œ")

        valid_companies_mask = (
            df['íšŒì‚¬_í•œê¸€_ì´ë¦„'].notna() & (df['íšŒì‚¬_í•œê¸€_ì´ë¦„'].str.strip() != '') &
            df['job_posting_url'].notna() & (df['job_posting_url'].str.strip() != '')
        )

        invalid_count = len(df) - valid_companies_mask.sum()
        if invalid_count > 0:
            self.logger.info(f"URLì´ ì—†ì–´ ì „ì²˜ë¦¬ì—ì„œ ì œì™¸ë˜ëŠ” íšŒì‚¬: {invalid_count}ê°œ (ì‹œíŠ¸ì—ì„œëŠ” ìœ ì§€ë¨)")

        self.logger.info(f"ì „ì²˜ë¦¬ ëŒ€ìƒ íšŒì‚¬ (URL í¬í•¨): {valid_companies_mask.sum()}ê°œ")

        if valid_companies_mask.any():
            self._fill_missing_selenium_required(df, valid_companies_mask)

        companies_to_process = df[
            valid_companies_mask &
            (df['selector'].isna() | (df['selector'] == '')) &
            (df['original_selector'].isna() | (df['original_selector'] == '')) &
            (df['selenium_required'] != -1)
        ]

        if companies_to_process.empty:
            self.logger.info("ìƒˆë¡œ ì „ì²˜ë¦¬í•  íšŒì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return df

        self.logger.info(f"{len(companies_to_process)}ê°œ íšŒì‚¬ì— ëŒ€í•œ ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

        existing_selectors = self._get_existing_selectors(df)
        self.logger.info(f"ê¸°ì¡´ íšŒì‚¬ë“¤ì—ì„œ ì‚¬ìš© ì¤‘ì¸ ì„ íƒì {len(existing_selectors)}ê°œë¥¼ ìš°ì„  ì ìš©í•©ë‹ˆë‹¤.")

        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            args_list = [(index, row, existing_selectors) for index, row in companies_to_process.iterrows()]
            results = executor.map(self._process_company_preprocess, args_list)

            for index, new_selector, selenium_status in results:
                if new_selector:
                    df.loc[index, 'selector'] = new_selector
                    self.logger.info(f"  - ì„ íƒì ì ìš© ì„±ê³µ: {new_selector}")
                if selenium_status is not None:
                    df.loc[index, 'selenium_required'] = selenium_status

        return df

    def stabilize_selectors(self, df: pd.DataFrame) -> pd.DataFrame:
        changed = False
        for index, row in df.iterrows():
            selector = row['selector']
            original_selector = row.get('original_selector', '')

            if (pd.isna(selector) or selector == '') and pd.notna(original_selector) and original_selector != '':
                stabilized = stabilize_selector(original_selector, conservative=False)
                df.loc[index, 'selector'] = stabilized
                self.logger.info(f"- {row['íšŒì‚¬_í•œê¸€_ì´ë¦„']} original_selectorë¥¼ ì•ˆì •í™”í•˜ì—¬ ì ìš©: {original_selector} -> {stabilized}")
                changed = True
            elif pd.notna(selector) and selector != '':
                stabilized = stabilize_selector(selector, conservative=True)
                if selector != stabilized:
                    df.loc[index, 'selector'] = stabilized
                    self.logger.info(f"- {row['íšŒì‚¬_í•œê¸€_ì´ë¦„']} ì„ íƒì ì•ˆì •í™”: {selector} -> {stabilized}")
                    changed = True
        if not changed:
            self.logger.info("ì•ˆì •í™”í•  ì„ íƒìê°€ ì—†ìŠµë‹ˆë‹¤.")
        return df


    def _get_existing_selectors(self, df: pd.DataFrame) -> List[str]:
        """ê¸°ì¡´ì— ì„±ê³µì ìœ¼ë¡œ ì‚¬ìš©ëœ ì„ íƒìë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        existing_selectors = []

        valid_selectors = df[df['selector'].notna() & (df['selector'] != '')]['selector'].unique()
        existing_selectors.extend(valid_selectors)

        expanded_selectors = []
        for selector in valid_selectors:
            expanded_selectors.append(selector)
            stabilized = stabilize_selector(selector, conservative=False)
            if stabilized != selector and stabilized:
                expanded_selectors.append(stabilized)

        known_good_selectors = [
            "a div.sc-9b56f69e-0.jlntFl",
            "div.JobPostingsJobPosting__Layout-sc-6ae888f2-0.ffnSOB div.JobPostingsJobPosting__Bottom-sc-6ae888f2-5.iXrIoX",
            "#jobList > div.jobList_info > div > a > span.title",
            "div.RecruitList_left__5MzDR div.RecruitList_title-wrapper__Gvh1r p",
            "div.swiper-slide button p",
            "button div p",
            "li.job-item a",
            "td.job-title a",
            ".job-list li a",
            ".career-item .title",
        ]
        expanded_selectors.extend(known_good_selectors)

        selector_counts = df[df['selector'].notna() & (df['selector'] != '')]['selector'].value_counts()
        sorted_selectors = selector_counts.index.tolist()

        final_selectors = sorted_selectors + [s for s in expanded_selectors if s not in sorted_selectors]

        self.logger.info(f"ìˆ˜ì§‘ëœ ì„ íƒì {len(final_selectors)}ê°œ (ê¸°ì¡´: {len(sorted_selectors)}ê°œ, í™•ì¥: {len(expanded_selectors)}ê°œ)")
        return final_selectors

    def _is_specific_enough_selector(self, selector: str) -> bool:
        """ì„ íƒìê°€ ì¶©ë¶„íˆ êµ¬ì²´ì ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
        selector = selector.strip()
        if not selector:
            return False

        parts = selector.split()

        if len(parts) == 1:
            part = parts[0].lower()
            if '.' in part or '#' in part:
                return True
            if '[' in part and ']' in part:
                if part == 'a[href]' or part.endswith('[href]') :
                    return False
                return True
            if ':' in part:
                return True
            return False

        if len(parts) >= 2:
            return True

        return False

    def _try_existing_selectors(self, soup: BeautifulSoup, existing_selectors: List[str], _: str) -> Optional[str]:
        """ê¸°ì¡´ ì„ íƒìë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹œë„í•´ì„œ ìœ íš¨í•œ ê²ƒì„ ì°¾ìŠµë‹ˆë‹¤."""
        for i, selector in enumerate(existing_selectors):
            if not self._is_specific_enough_selector(selector):
                continue

            try:
                elements = soup.select(selector)
                if not elements:
                    continue

                titles = [elem.get_text(strip=True) for elem in elements]
                valid_titles = [title for title in titles if title and len(title) > 3]

                if len(valid_titles) >= 1:
                    job_related_titles = [title for title in valid_titles
                                        if self.selector_analyzer._is_potential_job_posting(title)]

                    if len(job_related_titles) >= 1:
                        quality_score = len(job_related_titles) / len(valid_titles)

                        if len(valid_titles) > 50:
                            if quality_score < 0.8:
                                continue
                        elif len(valid_titles) > 20:
                            if quality_score < 0.6:
                                continue
                        elif len(valid_titles) > 5:
                            if quality_score < 0.4:
                                continue

                        category = "ê¸°ì¡´" if i < 10 else "í™•ì¥" if i < 50 else "íŒ¨í„´"
                        self.logger.info(f"  - {category} ì„ íƒì '{selector}' ê²€ì¦ ì„±ê³µ (ì±„ìš©ê³µê³ : {len(job_related_titles)}ê°œ/{len(valid_titles)}ê°œ, í’ˆì§ˆ: {quality_score:.1%})")
                        if len(valid_titles) <= 5:
                            for title in valid_titles[:3]:
                                self.logger.info(f"    ì˜ˆì‹œ: {title[:50]}...")
                        return selector

            except Exception:
                continue

        return None

    def _fill_missing_selenium_required(self, df: pd.DataFrame, mask: pd.Series):
        """selenium_required ê°’ì´ ì—†ëŠ” íšŒì‚¬ë“¤ì„ ìë™ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤. (ë³‘ë ¬ ì²˜ë¦¬)"""
        missing_selenium_mask = mask & (
            df['selenium_required'].isna() |
            (df['selenium_required'] == '') |
            (~df['selenium_required'].isin([0, 1, -1]))
        )
        missing_selenium = df[missing_selenium_mask]

        if missing_selenium.empty:
            self.logger.info("ëª¨ë“  íšŒì‚¬ì˜ selenium_required ê°’ì´ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return

        self.logger.info(f"{len(missing_selenium)}ê°œ íšŒì‚¬ì˜ selenium_required ê°’ì„ ë³‘ë ¬ë¡œ ìë™ ì„¤ì • ì¤‘...")

        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            future_to_index = {
                executor.submit(self._determine_selenium_requirement, row['job_posting_url'], row['íšŒì‚¬_í•œê¸€_ì´ë¦„']): index
                for index, row in missing_selenium.iterrows()
            }

            for future in concurrent.futures.as_completed(future_to_index):
                index = future_to_index[future]
                company_name = missing_selenium.loc[index, 'íšŒì‚¬_í•œê¸€_ì´ë¦„']
                try:
                    selenium_required = future.result()
                    df.loc[index, 'selenium_required'] = int(selenium_required)

                    selenium_text = "Selenium í•„ìš”" if selenium_required else "requests ì‚¬ìš©"
                    self.logger.info(f"  - {company_name}: {selenium_text}")
                except Exception as e:
                    self.logger.error(f"  - {company_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    df.loc[index, 'selenium_required'] = 1  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’

        self.logger.info(f"{len(missing_selenium)}ê°œ íšŒì‚¬ì˜ selenium_required ê°’ ì„¤ì • ì™„ë£Œ.")
    
    def _determine_selenium_requirement(self, url: str, _: str) -> int:
        """URLì„ ê¸°ë°˜ìœ¼ë¡œ Selenium í•„ìš” ì—¬ë¶€ë¥¼ ë™ì ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤."""

        try:
            selenium_req = self.selenium_checker.check_selenium_requirement(url)
            result = int(selenium_req)
            self.logger.info(f"    ğŸ” Selenium ì²´í¬ ê²°ê³¼: {url} -> {result}")
            return result
        except Exception as e:
            self.logger.info(f"    âš ï¸ Selenium ì²´í¬ ì‹¤íŒ¨ ({url}): {e} -> ê¸°ë³¸ê°’ 1 ì‚¬ìš©")
            return 1

    def _crawl_company(self, args):
        row, df_crawl = args
        company_name, url, use_selenium, selector = row['íšŒì‚¬_í•œê¸€_ì´ë¦„'], row['job_posting_url'], row['selenium_required'], row['selector']
        self.company_urls[company_name] = url
        self.logger.info(f"- {company_name} í¬ë¡¤ë§ ì¤‘...")

        driver = self.create_minimal_driver() if use_selenium else None
        html_content = self.get_html_content_for_crawling(url, use_selenium, driver, selector)
        if driver:
            driver.quit()

        if not html_content:
            return None, {'company': company_name, 'reason': 'HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨', 'url': url}

        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            postings = soup.select(selector)
            if not postings:
                return None, {'company': company_name, 'reason': f'ì„ íƒì \'{selector}\'ë¡œ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í•¨', 'url': url}

            job_titles = {post.get_text(strip=True) for post in postings if post.get_text(strip=True).strip()}
            if job_titles:
                self.logger.info(f"  - ì„±ê³µ: {len(job_titles)}ê°œ ê³µê³  ì¶”ì¶œ")
                return company_name, job_titles
            else:
                return None, {'company': company_name, 'reason': 'ìœ íš¨í•œ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í•¨', 'url': url}
        except Exception as e:
            return None, {'company': company_name, 'reason': f'HTML íŒŒì‹± ì‹¤íŒ¨: {e}', 'url': url}

    def crawl_jobs(self, df_config: pd.DataFrame) -> Tuple[Dict[str, Set[str]], List[Dict]]:
        self.logger.info("--- 3. ì±„ìš© ê³µê³  í¬ë¡¤ë§ ì‹œì‘ ---")
        df_crawl = df_config[
            (df_config['íšŒì‚¬_í•œê¸€_ì´ë¦„'].notna() & (df_config['íšŒì‚¬_í•œê¸€_ì´ë¦„'].str.strip() != '')) &
            (df_config['job_posting_url'].notna() & (df_config['job_posting_url'].str.strip() != '')) &
            (df_config['selector'].notna() & (df_config['selector'] != '')) &
            (df_config['selenium_required'] != -1)
        ].copy()

        if df_crawl.empty:
            self.logger.warning("í¬ë¡¤ë§í•  íšŒì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}, []

        current_jobs = {}
        failed_companies = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            args_list = [(row, df_crawl) for _, row in df_crawl.iterrows()]
            results = executor.map(self._crawl_company, args_list)

            for result in results:
                company_name, job_titles = result
                if company_name and job_titles:
                    current_jobs[company_name] = job_titles
                elif job_titles:
                    failed_companies.append(job_titles)

        self.logger.info("--- 3. ì±„ìš© ê³µê³  í¬ë¡¤ë§ ì¢…ë£Œ ---")
        return current_jobs, failed_companies

    def compare_and_notify(self, current_jobs: Dict, failed_companies: List):
        self.logger.info("--- 4. ë¹„êµ ë° ì•Œë¦¼ ì‹œì‘ ---")
        existing_jobs = self.load_existing_jobs()
        new_jobs = self.find_new_jobs(current_jobs, existing_jobs)
        warnings = self.check_suspicious_results(current_jobs, existing_jobs, new_jobs)
        self.send_slack_notification(new_jobs, warnings, failed_companies)
        if current_jobs:
            self.save_jobs(current_jobs)
        self.logger.info("--- 4. ë¹„êµ ë° ì•Œë¦¼ ì¢…ë£Œ ---")

    def get_html_content(self, url, use_selenium, driver=None, selector=None):
        """ì„ íƒì ë¶„ì„ìš© HTML ê°€ì ¸ì˜¤ê¸° ë©”ì„œë“œ (HTML íŒŒì¼ ì €ì¥ìš©)"""
        max_retries = 2
        for attempt in range(max_retries):
            try:
                if not use_selenium:
                    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=20)
                    response.raise_for_status()
                    return response.text
                else:
                    if driver is None: raise Exception("Selenium Driverê°€ ì—†ìŠµë‹ˆë‹¤.")
                    driver.get(url)

                    if selector:
                        from selenium.webdriver.common.by import By
                        from selenium.webdriver.support.ui import WebDriverWait
                        from selenium.webdriver.support import expected_conditions as EC
                        try:
                            WebDriverWait(driver, 20).until(
                                EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                            )
                            time.sleep(3)
                        except Exception:
                            self.logger.warning(f"''{selector}'' ìš”ì†Œë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                    else:
                        time.sleep(5)

                    return driver.page_source
            except Exception as e:
                if "timeout" in str(e).lower() and attempt < max_retries - 1:
                    self.logger.warning(f"í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ ({attempt + 1}/{max_retries}): {url} - ì¬ì‹œë„ ì¤‘...")
                    time.sleep(5)
                    continue
                else:
                    self.logger.error(f"HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}")
                    return None

    def get_html_content_for_crawling(self, url, use_selenium, driver=None, selector=None):
        """ì‹¤ì œ í¬ë¡¤ë§ìš© HTML ê°€ì ¸ì˜¤ê¸° ë©”ì„œë“œ (selenium_required ê°’ì— ë”°ë¼ ë¶„ê¸°)"""
        try:
            if not use_selenium:
                response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=20)
                response.raise_for_status()
                return response.text
            else:
                if driver is None:
                    raise Exception("Selenium Driverê°€ ì—†ìŠµë‹ˆë‹¤.")

                max_retries = 2
                for attempt in range(max_retries):
                    try:
                        driver.get(url)

                        if selector:
                            from selenium.webdriver.common.by import By
                            from selenium.webdriver.support.ui import WebDriverWait
                            from selenium.webdriver.support import expected_conditions as EC
                            try:
                                WebDriverWait(driver, 20).until(
                                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                                )
                                time.sleep(3)
                            except Exception:
                                self.logger.warning(f"ì„ íƒì '{selector}' ìš”ì†Œë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        else:
                            time.sleep(5)

                        return driver.page_source

                    except Exception as e:
                        if "timeout" in str(e).lower() and attempt < max_retries - 1:
                            self.logger.warning(f"í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ ({attempt + 1}/{max_retries}): {url} - ì¬ì‹œë„ ì¤‘...")
                            time.sleep(5)
                            continue
                        else:
                            raise e

        except Exception as e:
            self.logger.error(f"í¬ë¡¤ë§ìš© HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}")
            return None

    def create_minimal_driver(self):
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        driver = webdriver.Chrome(service=Service(executable_path="/usr/bin/chromedriver"), options=chrome_options)
        driver.set_page_load_timeout(20)
        return driver

    def load_existing_jobs(self) -> Dict[str, Set[str]]:
        if not os.path.exists(self.results_path):
            return {}
        try:
            df = pd.read_csv(self.results_path, encoding='utf-8-sig')
            return {comp: set(df_comp['job_posting_title']) for comp, df_comp in df.groupby('íšŒì‚¬_í•œê¸€_ì´ë¦„')}
        except Exception as e:
            self.logger.error(f"ê¸°ì¡´ ê³µê³  ë¡œë“œ ì˜¤ë¥˜: {e}")
            return {}

    def find_new_jobs(self, current_jobs: Dict, existing_jobs: Dict) -> Dict[str, List[str]]:
        new_jobs = {comp: list(curr - existing_jobs.get(comp, set())) for comp, curr in current_jobs.items()}
        return {c: j for c, j in new_jobs.items() if j}

    def check_suspicious_results(self, current_jobs: Dict, existing_jobs: Dict, new_jobs: Dict) -> List[str]:
        warnings = []
        for company, new_list in new_jobs.items():
            if len(existing_jobs.get(company, set())) > 0 and len(new_list) == len(current_jobs.get(company, set())):
                warnings.append(f"{company}: ëª¨ë“  ê³µê³ ê°€ ì‹ ê·œì…ë‹ˆë‹¤. ì „ì²´ ì±„ìš© í˜ì´ì§€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return warnings

    def save_jobs(self, current_jobs: Dict):
        kst = pytz.timezone('Asia/Seoul')
        current_time_kst = datetime.now(kst)
        all_postings = [{'íšŒì‚¬_í•œê¸€_ì´ë¦„': comp, 'job_posting_title': title, 'crawl_datetime': current_time_kst.strftime('%Y-%m-%d %H:%M:%S')} for comp, titles in current_jobs.items() for title in titles]
        pd.DataFrame(all_postings).to_csv(self.results_path, index=False, encoding='utf-8-sig')
        self.logger.info(f"ê²°ê³¼ë¥¼ '{self.results_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    def send_slack_notification(self, new_jobs: Dict, warnings: List, failed_companies: List):
        if not self.webhook_url:
            self.logger.error(f"{self.webhook_url_env}ì´ .envì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        if not new_jobs and not warnings and not failed_companies:
            self.logger.info("ì•Œë¦¼ ë³´ë‚¼ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        kst = pytz.timezone('Asia/Seoul')
        current_time = datetime.now(kst).strftime('%H:%M')
        messages_to_send = []

        if new_jobs:
            total_new_jobs = sum(len(jobs) for jobs in new_jobs.values())
            header_msg = f"ğŸ‰ *ìƒˆë¡œìš´ ì±„ìš©ê³µê³  {total_new_jobs}ê°œ ë°œê²¬!* ({current_time})\n"

            current_message = header_msg
            for company, jobs in new_jobs.items():
                company_url = self.company_urls.get(company, "")
                linked_company = f"<{company_url}|{company}>" if company_url else company
                company_section = f"\nğŸ“¢ *{linked_company}* - {len(jobs)}ê°œ\n"

                for job in jobs:
                    company_section += f"â€¢ {job}\n"

                if len(current_message + company_section) > 3500:
                    messages_to_send.append(current_message.strip())
                    current_message = company_section
                else:
                    current_message += company_section

            if current_message.strip():
                messages_to_send.append(current_message.strip())

        if warnings:
            warning_msg = "âš ï¸ *ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê²°ê³¼* (ì§ì ‘ í™•ì¸ í•„ìš”)\n"
            for warning in warnings:
                warning_msg += f"â€¢ {warning}\n"
            messages_to_send.append(warning_msg.strip())

        if failed_companies:
            fail_msg = "âŒ *í¬ë¡¤ë§ ì‹¤íŒ¨*\n"
            for fail in failed_companies:
                fail_line = f"â€¢ {fail['company']}: {fail['reason']}\n"
                if len(fail_msg + fail_line) > 3500:
                    messages_to_send.append(fail_msg.strip())
                    fail_msg = f"âŒ *í¬ë¡¤ë§ ì‹¤íŒ¨ (ê³„ì†)*\n{fail_line}"
                else:
                    fail_msg += fail_line

            if fail_msg.strip() != "âŒ *í¬ë¡¤ë§ ì‹¤íŒ¨*":
                messages_to_send.append(fail_msg.strip())

        for i, message in enumerate(messages_to_send):
            payload = {"text": message, "username": "ì±„ìš©ê³µê³  ì•Œë¦¬ë¯¸", "icon_emoji": ":robot_face:"}
            try:
                self.logger.info(f"ğŸ“¤ ìŠ¬ë™ ë©”ì‹œì§€ ì „ì†¡ ì‹œë„ ({i+1}/{len(messages_to_send)}) - ë©”ì‹œì§€ ê¸¸ì´: {len(message)}ì")
                response = requests.post(self.webhook_url, json=payload, timeout=15)

                if response.status_code == 200:
                    self.logger.info(f"âœ… ìŠ¬ë™ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ ({i+1}/{len(messages_to_send)})")
                else:
                    self.logger.error(f"âŒ ìŠ¬ë™ ì‘ë‹µ ì˜¤ë¥˜ ({i+1}/{len(messages_to_send)}): {response.status_code} - {response.text}")

                import time
                time.sleep(1)

            except Exception as e:
                self.logger.error(f"âŒ ìŠ¬ë™ ì•Œë¦¼ ì „ì†¡ ì˜¤ë¥˜ ({i+1}/{len(messages_to_send)}): {e}")

def main():
    base_dir = os.path.dirname(os.path.abspath(__file__))
    dag = JobMonitoringDAG(base_dir)
    dag.run()

if __name__ == "__main__":
    main()
