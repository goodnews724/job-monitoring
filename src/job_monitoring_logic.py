import os
import time
import pandas as pd
import requests
import re
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime
import logging
from typing import Dict, List, Set, Tuple, Optional
from dotenv import load_dotenv
from google_sheet_utils import GoogleSheetManager
from analyze_titles import JobPostingSelectorAnalyzer
from utils import stabilize_selector, SeleniumRequirementChecker

load_dotenv()

class JobMonitoringDAG:
    def __init__(self, base_dir: str, worksheet_name: str = '[ë“±ë¡]ì±„ìš©í™ˆí˜ì´ì§€ ëª¨ìŒ', webhook_url_env: str = 'SLACK_WEBHOOK_URL', results_filename: str = 'job_postings_latest.csv'):
        self.base_dir = base_dir
        self.data_dir = os.path.join(base_dir, 'data')
        self.html_dir = os.path.join(base_dir, 'html')
        # HTML ë””ë ‰í† ë¦¬ ìƒì„± (ë” ì•ˆì „í•œ ë°©ì‹)
        try:
            os.makedirs(self.html_dir, exist_ok=True)
        except (FileExistsError, OSError):
            # ì´ë¯¸ ì¡´ì¬í•˜ê±°ë‚˜ ê¶Œí•œ ë¬¸ì œê°€ ìˆì–´ë„ ê³„ì† ì§„í–‰
            pass
        self.worksheet_name = worksheet_name
        self.webhook_url_env = webhook_url_env  # í™˜ê²½ë³€ìˆ˜ ì´ë¦„ ì €ì¥
        self.results_path = os.path.join(self.data_dir, results_filename)
        self.webhook_url = os.getenv(webhook_url_env)
        self.company_urls = {}

        self._setup_logging()
        self.sheet_manager = GoogleSheetManager(base_dir)
        self.selenium_checker = SeleniumRequirementChecker()
        self.selector_analyzer = JobPostingSelectorAnalyzer()

    def _setup_logging(self):
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)

        # ì´ ë¡œê±°ë‚˜ ë£¨íŠ¸ ë¡œê±°ì— í•¸ë“¤ëŸ¬ê°€ ì—†ì„ ë•Œë§Œ í•¸ë“¤ëŸ¬ë¥¼ ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
        if not self.logger.handlers and not logging.getLogger().handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)

    def run(self):
        self.logger.info(f"ğŸš€ Job Monitoring DAG ì‹œì‘ - {self.worksheet_name}")
        df_config = self.sheet_manager.get_all_records_as_df(self.worksheet_name)
        if df_config.empty:
            self.logger.error(f"Google Sheetsì—ì„œ ì„¤ì • ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {self.worksheet_name}")
            return

        original_df_config = df_config.copy()

        df_config = self.preprocess_companies(df_config)
        df_config = self.stabilize_selectors(df_config)

        if not df_config.equals(original_df_config):
            self.logger.info("Google Sheetsì— ë³€ê²½ ì‚¬í•­ ì—…ë°ì´íŠ¸ ì¤‘...")
            self.sheet_manager.update_sheet_from_df(df_config, self.worksheet_name)
        else:
            self.logger.info("ì„¤ì • ë³€ê²½ ì‚¬í•­ì´ ì—†ì–´ Google Sheets ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

        current_jobs, failed_companies = self.crawl_jobs(df_config)
        self.compare_and_notify(current_jobs, failed_companies)
        self.logger.info(f"âœ… Job Monitoring DAG ì¢…ë£Œ - {self.worksheet_name}")

    def preprocess_companies(self, df: pd.DataFrame) -> pd.DataFrame:
        self.logger.info("--- 1. ì „ì²˜ë¦¬ ì‹œì‘ ---")

        # ë¹ˆ íšŒì‚¬ ì´ë¦„ê³¼ ë¹ˆ job_posting_urlì„ ê°€ì§„ í–‰ë“¤ í•„í„°ë§
        df = df[
            df['íšŒì‚¬_í•œê¸€_ì´ë¦„'].notna() & (df['íšŒì‚¬_í•œê¸€_ì´ë¦„'].str.strip() != '') &
            df['job_posting_url'].notna() & (df['job_posting_url'].str.strip() != '')
        ]
        self.logger.info(f"ìœ íš¨í•œ íšŒì‚¬ ë°ì´í„° (URL í¬í•¨): {len(df)}ê°œ")

        # 1. ë¨¼ì € ëª¨ë“  íšŒì‚¬ì˜ selenium_required ê°’ì„ ìë™ ì±„ìš°ê¸°
        self._fill_missing_selenium_required(df)

        # 2. selectorê°€ ì—†ê³  original_selectorë„ ì—†ìœ¼ë©°, selenium_requiredê°€ -1ì´ ì•„ë‹Œ íšŒì‚¬ë“¤ë§Œ ì²˜ë¦¬
        companies_to_process = df[
            (df['selector'].isna() | (df['selector'] == '')) &
            (df['original_selector'].isna() | (df['original_selector'] == '')) &
            (df['selenium_required'] != -1)  # HTML ì €ì¥ ì‹¤íŒ¨ë¡œ ìŠ¤í‚µëœ íšŒì‚¬ëŠ” ì œì™¸
        ]

        if companies_to_process.empty:
            self.logger.info("ìƒˆë¡œ ì „ì²˜ë¦¬í•  íšŒì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return df

        self.logger.info(f"{len(companies_to_process)}ê°œ íšŒì‚¬ì— ëŒ€í•œ ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

        # 3. ë‹¤ë¥¸ íšŒì‚¬ë“¤ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ì‚¬ìš©ëœ ì„ íƒìë“¤ ìˆ˜ì§‘
        existing_selectors = self._get_existing_selectors(df)
        self.logger.info(f"ê¸°ì¡´ íšŒì‚¬ë“¤ì—ì„œ ì‚¬ìš© ì¤‘ì¸ ì„ íƒì {len(existing_selectors)}ê°œë¥¼ ìš°ì„  ì ìš©í•©ë‹ˆë‹¤.")

        driver = self.create_minimal_driver()

        for index, row in companies_to_process.iterrows():
            company_name = row['íšŒì‚¬_í•œê¸€_ì´ë¦„']
            url = row['job_posting_url']
            self.logger.info(f"- {company_name} ì²˜ë¦¬ ì¤‘...")

            # HTML ê°€ì ¸ì˜¤ê¸° (ë©”ëª¨ë¦¬ì—ì„œë§Œ ì²˜ë¦¬)
            html_content = self.get_html_content(url, df.loc[index, 'selenium_required'], driver)
            if html_content:
                self.logger.info(f"  - HTML ê°€ì ¸ì˜¤ê¸° ì„±ê³µ")

                # ì„ íƒì ë¶„ì„ - ê¸°ì¡´ ì„ íƒì ìš°ì„  ì‹œë„
                soup = BeautifulSoup(html_content, 'html.parser')
                found_selector = self._try_existing_selectors(soup, existing_selectors, company_name)

                if found_selector:
                    df.loc[index, 'selector'] = found_selector
                    self.logger.info(f"  - ê¸°ì¡´ ì„ íƒì ì ìš© ì„±ê³µ: {found_selector}")
                else:
                    # ê¸°ì¡´ ì„ íƒìë¡œ ì•ˆ ë˜ë©´ ìƒˆë¡œ ë¶„ì„
                    best_selector, _ = self.selector_analyzer.find_best_selector(soup)
                    if best_selector:
                        df.loc[index, 'selector'] = best_selector
                        existing_selectors.append(best_selector)  # ìƒˆ ì„ íƒìë¥¼ ëª©ë¡ì— ì¶”ê°€
                        self.logger.info(f"  - ìƒˆë¡œìš´ ì„ íƒì ë¶„ì„ ì„±ê³µ: {best_selector}")
                    else:
                        self.logger.warning("  - ì„ íƒì ë¶„ì„ ì‹¤íŒ¨")
            else:
                # HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ì‹œ selenium_requiredë¥¼ -1ë¡œ ì„¤ì •í•˜ì—¬ í–¥í›„ ì‹œë„í•˜ì§€ ì•ŠìŒ
                df.loc[index, 'selenium_required'] = -1
                self.logger.error(f"  - HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {company_name} (selenium_requiredë¥¼ -1ë¡œ ì„¤ì •)")

        if driver:
            driver.quit()
        self.logger.info("--- 1. ì „ì²˜ë¦¬ ì¢…ë£Œ ---")
        return df

    def stabilize_selectors(self, df: pd.DataFrame) -> pd.DataFrame:
        self.logger.info("--- 2. ì„ íƒì ì•ˆì •í™” ì‹œì‘ ---")
        changed = False
        for index, row in df.iterrows():
            selector = row['selector']
            original_selector = row.get('original_selector', '')

            # selectorê°€ ë¹„ì–´ìˆê³  original_selectorê°€ ìˆìœ¼ë©´ original_selectorë¥¼ ì•ˆì •í™”í•´ì„œ ì‚¬ìš©
            if (pd.isna(selector) or selector == '') and pd.notna(original_selector) and original_selector != '':
                stabilized = stabilize_selector(original_selector, conservative=False)
                df.loc[index, 'selector'] = stabilized
                self.logger.info(f"- {row['íšŒì‚¬_í•œê¸€_ì´ë¦„']} original_selectorë¥¼ ì•ˆì •í™”í•˜ì—¬ ì ìš©: {original_selector} -> {stabilized}")
                changed = True
            # selectorê°€ ìˆìœ¼ë©´ ê¸°ì¡´ ë¡œì§ëŒ€ë¡œ ì•ˆì •í™”
            elif pd.notna(selector) and selector != '':
                stabilized = stabilize_selector(selector, conservative=True)
                if selector != stabilized:
                    df.loc[index, 'selector'] = stabilized
                    self.logger.info(f"- {row['íšŒì‚¬_í•œê¸€_ì´ë¦„']} ì„ íƒì ì•ˆì •í™”: {selector} -> {stabilized}")
                    changed = True
        if not changed:
            self.logger.info("ì•ˆì •í™”í•  ì„ íƒìê°€ ì—†ìŠµë‹ˆë‹¤.")
        self.logger.info("--- 2. ì„ íƒì ì•ˆì •í™” ì¢…ë£Œ ---")
        return df


    def _get_existing_selectors(self, df: pd.DataFrame) -> List[str]:
        """ê¸°ì¡´ì— ì„±ê³µì ìœ¼ë¡œ ì‚¬ìš©ëœ ì„ íƒìë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        existing_selectors = []

        # 1. í˜„ì¬ DataFrameì—ì„œ ìœ íš¨í•œ ì„ íƒìë“¤ ìˆ˜ì§‘
        valid_selectors = df[df['selector'].notna() & (df['selector'] != '')]['selector'].unique()
        existing_selectors.extend(valid_selectors)

        # 2. ê° ì„ íƒìì˜ ì›ë³¸ê³¼ ì•ˆì •í™”ëœ ë²„ì „ ë‘˜ ë‹¤ ì¶”ê°€
        expanded_selectors = []
        for selector in valid_selectors:
            expanded_selectors.append(selector)  # ì›ë³¸
            stabilized = stabilize_selector(selector, conservative=False)  # ì•ˆì •í™”ëœ ë²„ì „
            if stabilized != selector and stabilized:
                expanded_selectors.append(stabilized)

        # 3. ì•Œë ¤ì§„ ì„±ê³µ ì„ íƒìë“¤ ì¶”ê°€ (ì •ë‹µ ê¸°ë°˜) - êµ¬ì²´ì ì¸ ê²ƒë“¤ë§Œ
        known_good_selectors = [
            "a div.sc-9b56f69e-0.jlntFl",  # greetinghr ê³„ì—´
            "div.JobPostingsJobPosting__Layout-sc-6ae888f2-0.ffnSOB div.JobPostingsJobPosting__Bottom-sc-6ae888f2-5.iXrIoX",  # ninehire ê³„ì—´
            "#jobList > div.jobList_info > div > a > span.title",  # íŠ¸ë¦¬ë…¸ë“œ ìŠ¤íƒ€ì¼
            "div.RecruitList_left__5MzDR div.RecruitList_title-wrapper__Gvh1r p",  # recruiter.co.kr ê³„ì—´
            "div.swiper-slide button p",  # ìŠ¬ë¼ì´ë” ë‚´ë¶€ ë²„íŠ¼
            "button div p",  # ë²„íŠ¼ ë‚´ë¶€ í…ìŠ¤íŠ¸
            "li.job-item a",  # êµ¬ì²´ì ì¸ ë¦¬ìŠ¤íŠ¸ ë§í¬
            "td.job-title a",  # êµ¬ì²´ì ì¸ í…Œì´ë¸” ì…€
            ".job-list li a",  # ì±„ìš© ëª©ë¡ ë§í¬
            ".career-item .title",  # ì±„ìš© ì•„ì´í…œ ì œëª©
        ]
        expanded_selectors.extend(known_good_selectors)

        # 4. ì„ íƒì ì‚¬ìš© ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬ (ë§ì´ ì‚¬ìš©ëœ ì„ íƒìë¶€í„° ì‹œë„)
        selector_counts = df[df['selector'].notna() & (df['selector'] != '')]['selector'].value_counts()
        sorted_selectors = selector_counts.index.tolist()

        # 5. ìµœì¢… ë¦¬ìŠ¤íŠ¸: ë¹ˆë„ìˆœ ì„ íƒì + í™•ì¥ëœ ì„ íƒìë“¤
        final_selectors = sorted_selectors + [s for s in expanded_selectors if s not in sorted_selectors]

        self.logger.info(f"ìˆ˜ì§‘ëœ ì„ íƒì {len(final_selectors)}ê°œ (ê¸°ì¡´: {len(sorted_selectors)}ê°œ, í™•ì¥: {len(expanded_selectors)}ê°œ)")
        return final_selectors

    def _is_specific_enough_selector(self, selector: str) -> bool:
        """ì„ íƒìê°€ ì¶©ë¶„íˆ êµ¬ì²´ì ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
        selector = selector.strip()
        if not selector:
            return False

        # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ëœ ì„ íƒì ë¶€ë¶„ë“¤
        parts = selector.split()

        # 1. ë‹¨ì¼ íƒœê·¸ëª…ë§Œ ìˆëŠ” ê²½ìš°ëŠ” ë¹„êµ¬ì²´ì 
        if len(parts) == 1:
            part = parts[0].lower()
            # í´ë˜ìŠ¤ë‚˜ IDê°€ ìˆìœ¼ë©´ êµ¬ì²´ì 
            if '.' in part or '#' in part:
                return True
            # ì†ì„± ì„ íƒìê°€ ìˆëŠ”ì§€ ì²´í¬
            if '[' in part and ']' in part:
                # í•˜ì§€ë§Œ ë‹¨ìˆœíˆ a[href] ê°™ì€ ë„ˆë¬´ ì¼ë°˜ì ì¸ ê²ƒì€ ì œì™¸
                # ì†ì„±ê°’ì´ êµ¬ì²´ì ì¸ì§€ ì²´í¬ (ì˜ˆ: a[class="job-link"])
                if part == 'a[href]' or part.endswith('[href]'):
                    return False
                return True
            # ê°€ìƒ ì„ íƒìê°€ ìˆìœ¼ë©´ êµ¬ì²´ì 
            if ':' in part:
                return True
            # ë‹¨ìˆœ íƒœê·¸ëª…ë§Œ ìˆìœ¼ë©´ ë¹„êµ¬ì²´ì 
            return False

        # 2. ì—¬ëŸ¬ ë ˆë²¨ì˜ ì„ íƒìëŠ” êµ¬ì²´ì  (ìµœì†Œ 2ë ˆë²¨ ì´ìƒ)
        if len(parts) >= 2:
            return True

        return False

    def _try_existing_selectors(self, soup: BeautifulSoup, existing_selectors: List[str], _: str) -> Optional[str]:
        """ê¸°ì¡´ ì„ íƒìë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹œë„í•´ì„œ ìœ íš¨í•œ ê²ƒì„ ì°¾ìŠµë‹ˆë‹¤."""
        for i, selector in enumerate(existing_selectors):
            # ë„ˆë¬´ ì¼ë°˜ì ì¸ ì„ íƒìëŠ” ê±´ë„ˆë›°ê¸°
            if not self._is_specific_enough_selector(selector):
                continue

            try:
                elements = soup.select(selector)
                if not elements:
                    continue

                # ì„ íƒìë¡œ ì°¾ì€ ìš”ì†Œë“¤ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                titles = [elem.get_text(strip=True) for elem in elements]
                valid_titles = [title for title in titles if title and len(title) > 3]

                # ê¸°ë³¸ ê²€ì¦: ìµœì†Œ 1ê°œ ì´ìƒì˜ ìœ íš¨í•œ í…ìŠ¤íŠ¸
                if len(valid_titles) >= 1:
                    # ì±„ìš©ê³µê³  ê´€ë ¨ì„± ê²€ì‚¬
                    job_related_titles = [title for title in valid_titles
                                        if self.selector_analyzer._is_potential_job_posting(title)]

                    # ë” ì—„ê²©í•œ ê²€ì¦: ì ì–´ë„ 1ê°œ ì´ìƒì˜ ì±„ìš©ê³µê³ ê°€ ìˆì–´ì•¼ í•¨
                    if len(job_related_titles) >= 1:
                        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°: ì±„ìš©ê³µê³  ë¹„ìœ¨ì´ ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
                        quality_score = len(job_related_titles) / len(valid_titles)

                        # ë„ˆë¬´ ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì„ íƒìëŠ” ì¼ë°˜ì ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
                        if len(valid_titles) > 50:
                            # 50ê°œ ì´ìƒì´ë©´ í’ˆì§ˆì´ 80% ì´ìƒì´ì–´ì•¼ í•¨
                            if quality_score < 0.8:
                                continue
                        elif len(valid_titles) > 20:
                            # 20ê°œ ì´ìƒì´ë©´ í’ˆì§ˆì´ 60% ì´ìƒì´ì–´ì•¼ í•¨
                            if quality_score < 0.6:
                                continue
                        elif len(valid_titles) > 5:
                            # 5ê°œ ì´ìƒì´ë©´ í’ˆì§ˆì´ 40% ì´ìƒì´ì–´ì•¼ í•¨
                            if quality_score < 0.4:
                                continue
                        # 5ê°œ ì´í•˜ë©´ ì ì–´ë„ 1ê°œ ì´ìƒì˜ ì±„ìš©ê³µê³ ë§Œ ìˆìœ¼ë©´ ë¨

                        # ë¡œê·¸ì—ì„œ ì–´ë–¤ ì¹´í…Œê³ ë¦¬ì˜ ì„ íƒìì¸ì§€ í‘œì‹œ
                        category = "ê¸°ì¡´" if i < 10 else "í™•ì¥" if i < 50 else "íŒ¨í„´"
                        self.logger.info(f"  - {category} ì„ íƒì '{selector}' ê²€ì¦ ì„±ê³µ (ì±„ìš©ê³µê³ : {len(job_related_titles)}ê°œ/{len(valid_titles)}ê°œ, í’ˆì§ˆ: {quality_score:.1%})")
                        if len(valid_titles) <= 5:  # ì ì€ ìˆ˜ì¼ ë•Œë§Œ ìƒ˜í”Œ ì¶œë ¥
                            for title in valid_titles[:3]:
                                self.logger.info(f"    ì˜ˆì‹œ: {title[:50]}...")
                        return selector

            except Exception:
                # ì„ íƒì ë¬¸ë²• ì˜¤ë¥˜ ë“±ì€ ë¬´ì‹œí•˜ê³  ë‹¤ìŒ ì„ íƒì ì‹œë„
                continue

        return None

    def _fill_missing_selenium_required(self, df: pd.DataFrame) -> None:
        """selenium_required ê°’ì´ ì—†ëŠ” íšŒì‚¬ë“¤ì„ ìë™ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤."""
        # NaN, ë¹ˆ ë¬¸ìì—´, ë˜ëŠ” 0/1/-1ì´ ì•„ë‹Œ ê°’ë“¤ì„ ëª¨ë‘ ì²´í¬
        missing_selenium = df[
            df['selenium_required'].isna() |
            (df['selenium_required'] == '') |
            (~df['selenium_required'].isin([0, 1, -1]))
        ]
        
        if missing_selenium.empty:
            self.logger.info("ëª¨ë“  íšŒì‚¬ì˜ selenium_required ê°’ì´ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
            
        self.logger.info(f"{len(missing_selenium)}ê°œ íšŒì‚¬ì˜ selenium_required ê°’ì„ ìë™ ì„¤ì • ì¤‘...")
        
        # ìë™ íŒë‹¨ ê·œì¹™
        for index, row in missing_selenium.iterrows():
            company_name = row['íšŒì‚¬_í•œê¸€_ì´ë¦„']
            url = row['job_posting_url']
            
            selenium_required = self._determine_selenium_requirement(url, company_name)
            df.loc[index, 'selenium_required'] = selenium_required
            
            selenium_text = "Selenium í•„ìš”" if selenium_required else "requests ì‚¬ìš©"
            self.logger.info(f"  - {company_name}: {selenium_text}")
    
    def _determine_selenium_requirement(self, url: str, _: str) -> int:
        """URLì„ ê¸°ë°˜ìœ¼ë¡œ Selenium í•„ìš” ì—¬ë¶€ë¥¼ ë™ì ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤."""
        
        # ëª¨ë“  ì‚¬ì´íŠ¸ì— ëŒ€í•´ ì‹¤ì œë¡œ ì²´í¬í•´ì„œ íŒë‹¨
        try:
            selenium_req = self.selenium_checker.check_selenium_requirement(url)
            return int(selenium_req)
        except Exception:
            # ì²´í¬ ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ Selenium ì‚¬ìš©
            return 1

    def crawl_jobs(self, df_config: pd.DataFrame) -> Tuple[Dict[str, Set[str]], List[Dict]]:
        self.logger.info("--- 3. ì±„ìš© ê³µê³  í¬ë¡¤ë§ ì‹œì‘ ---")
        df_crawl = df_config[
            (df_config['selector'].notna() & (df_config['selector'] != '')) &
            (df_config['selenium_required'] != -1)  # HTML ì €ì¥ ì‹¤íŒ¨ë¡œ ìŠ¤í‚µëœ íšŒì‚¬ëŠ” ì œì™¸
        ].copy()
        if df_crawl.empty:
            self.logger.warning("í¬ë¡¤ë§í•  íšŒì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}, []
        driver = self.create_minimal_driver() if df_crawl['selenium_required'].any() else None
        current_jobs = {}
        failed_companies = []

        for _, row in df_crawl.iterrows():
            company_name, url, use_selenium, selector = row['íšŒì‚¬_í•œê¸€_ì´ë¦„'], row['job_posting_url'], row['selenium_required'], row['selector']
            self.company_urls[company_name] = url
            self.logger.info(f"- {company_name} í¬ë¡¤ë§ ì¤‘...")
            html_content = self.get_html_content_for_crawling(url, use_selenium, driver, selector)
            if not html_content:
                failed_companies.append({'company': company_name, 'reason': 'HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨', 'url': url})
                continue
            try:
                soup = BeautifulSoup(html_content, 'html.parser')
                postings = soup.select(selector)
                if not postings:
                    failed_companies.append({'company': company_name, 'reason': f'ì„ íƒì \'{selector}\'ë¡œ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í•¨', 'url': url})
                    continue
                job_titles = {post.get_text(strip=True) for post in postings 
                             if post.get_text(strip=True).strip()}  # ë¹ˆ í…ìŠ¤íŠ¸ë§Œ ì œì™¸
                if job_titles:
                    current_jobs[company_name] = job_titles
                    self.logger.info(f"  - ì„±ê³µ: {len(job_titles)}ê°œ ê³µê³  ì¶”ì¶œ")
                else:
                    failed_companies.append({'company': company_name, 'reason': 'ìœ íš¨í•œ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í•¨', 'url': url})
            except Exception as e:
                failed_companies.append({'company': company_name, 'reason': f'HTML íŒŒì‹± ì‹¤íŒ¨: {e}', 'url': url})

        if driver:
            driver.quit()
        self.logger.info("--- 3. ì±„ìš© ê³µê³  í¬ë¡¤ë§ ì¢…ë£Œ ---")
        return current_jobs, failed_companies

    def compare_and_notify(self, current_jobs: Dict, failed_companies: List):
        self.logger.info("--- 4. ë¹„êµ ë° ì•Œë¦¼ ì‹œì‘ ---")
        existing_jobs = self.load_existing_jobs()
        new_jobs = self.find_new_jobs(current_jobs, existing_jobs)
        warnings = self.check_suspicious_results(current_jobs, existing_jobs, new_jobs)
        self.send_slack_notification(new_jobs, warnings, failed_companies)
        if current_jobs:
            self.save_jobs(current_jobs)
        self.logger.info("--- 4. ë¹„êµ ë° ì•Œë¦¼ ì¢…ë£Œ ---")

    def get_html_content(self, url, use_selenium, driver=None, selector=None):
        """ì„ íƒì ë¶„ì„ìš© HTML ê°€ì ¸ì˜¤ê¸° ë©”ì„œë“œ (HTML íŒŒì¼ ì €ì¥ìš©)"""
        try:
            if not use_selenium:
                response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
                response.raise_for_status()
                return response.text
            else:
                if driver is None: raise Exception("Selenium Driverê°€ ì—†ìŠµë‹ˆë‹¤.")
                driver.get(url)

                if selector:
                    from selenium.webdriver.common.by import By
                    from selenium.webdriver.support.ui import WebDriverWait
                    from selenium.webdriver.support import expected_conditions as EC
                    try:
                        # 1. ì„ íƒìê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ìµœëŒ€ 7ì´ˆ ëŒ€ê¸°
                        WebDriverWait(driver, 7).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                        )
                        # 2. ë°ì´í„° ë¡œë“œë¥¼ ìœ„í•´ 1.5ì´ˆ ì¶”ê°€ ëŒ€ê¸°
                        time.sleep(1.5)
                    except Exception:
                        # ëŒ€ê¸° ì‹¤íŒ¨ ì‹œ, ê²½ê³ ë§Œ ë‚¨ê¸°ê³  HTMLì„ ë°”ë¡œ ê°€ì ¸ì˜´
                        self.logger.warning(f"''{selector}'' ìš”ì†Œë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                else:
                    # ì„ íƒìê°€ ì—†ìœ¼ë©´ 2ì´ˆ ê¸°ë³¸ ëŒ€ê¸°
                    time.sleep(2)

                return driver.page_source
        except Exception as e:
            self.logger.error(f"HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}")
            return None

    def get_html_content_for_crawling(self, url, use_selenium, driver=None, selector=None):
        """ì‹¤ì œ í¬ë¡¤ë§ìš© HTML ê°€ì ¸ì˜¤ê¸° ë©”ì„œë“œ (selenium_required ê°’ì— ë”°ë¼ ë¶„ê¸°)"""
        try:
            if not use_selenium:
                # requests ì‚¬ìš©
                response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=15)
                response.raise_for_status()
                return response.text
            else:
                # selenium ì‚¬ìš©
                if driver is None:
                    raise Exception("Selenium Driverê°€ ì—†ìŠµë‹ˆë‹¤.")

                # íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¬ì‹œë„ ë¡œì§
                max_retries = 2
                for attempt in range(max_retries):
                    try:
                        driver.get(url)

                        if selector:
                            from selenium.webdriver.common.by import By
                            from selenium.webdriver.support.ui import WebDriverWait
                            from selenium.webdriver.support import expected_conditions as EC
                            try:
                                # ì„ íƒìê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ìµœëŒ€ 15ì´ˆ ëŒ€ê¸° (10ì´ˆ â†’ 15ì´ˆ)
                                WebDriverWait(driver, 15).until(
                                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                                )
                                # ë°ì´í„° ë¡œë“œë¥¼ ìœ„í•´ 3ì´ˆ ì¶”ê°€ ëŒ€ê¸° (2ì´ˆ â†’ 3ì´ˆ)
                                time.sleep(3)
                            except Exception:
                                # ëŒ€ê¸° ì‹¤íŒ¨ ì‹œ, ê²½ê³ ë§Œ ë‚¨ê¸°ê³  HTMLì„ ë°”ë¡œ ê°€ì ¸ì˜´
                                self.logger.warning(f"ì„ íƒì '{selector}' ìš”ì†Œë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        else:
                            # ì„ íƒìê°€ ì—†ìœ¼ë©´ 5ì´ˆ ê¸°ë³¸ ëŒ€ê¸° (3ì´ˆ â†’ 5ì´ˆ)
                            time.sleep(5)

                        return driver.page_source

                    except Exception as e:
                        if "timeout" in str(e).lower() and attempt < max_retries - 1:
                            self.logger.warning(f"í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ ({attempt + 1}/{max_retries}): {url} - ì¬ì‹œë„ ì¤‘...")
                            time.sleep(2)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                            continue
                        else:
                            raise e

        except Exception as e:
            self.logger.error(f"í¬ë¡¤ë§ìš© HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}")
            return None

    def create_minimal_driver(self):
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        driver.set_page_load_timeout(15)
        return driver

    def load_existing_jobs(self) -> Dict[str, Set[str]]:
        if not os.path.exists(self.results_path):
            return {}
        try:
            df = pd.read_csv(self.results_path, encoding='utf-8-sig')
            return {comp: set(df_comp['job_posting_title']) for comp, df_comp in df.groupby('íšŒì‚¬_í•œê¸€_ì´ë¦„')}
        except Exception as e:
            self.logger.error(f"ê¸°ì¡´ ê³µê³  ë¡œë“œ ì˜¤ë¥˜: {e}")
            return {}

    def find_new_jobs(self, current_jobs: Dict, existing_jobs: Dict) -> Dict[str, List[str]]:
        new_jobs = {comp: list(curr - existing_jobs.get(comp, set())) for comp, curr in current_jobs.items()}
        return {c: j for c, j in new_jobs.items() if j}

    def check_suspicious_results(self, current_jobs: Dict, existing_jobs: Dict, new_jobs: Dict) -> List[str]:
        warnings = []
        for company, new_list in new_jobs.items():
            if len(existing_jobs.get(company, set())) > 0 and len(new_list) == len(current_jobs.get(company, set())):
                warnings.append(f"{company}: ëª¨ë“  ê³µê³ ê°€ ì‹ ê·œì…ë‹ˆë‹¤. ì „ì²´ ì±„ìš© í˜ì´ì§€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return warnings

    def save_jobs(self, current_jobs: Dict):
        all_postings = [{'íšŒì‚¬_í•œê¸€_ì´ë¦„': comp, 'job_posting_title': title, 'crawl_datetime': datetime.now().strftime('%Y-%m-%d %H:%M:%S')} for comp, titles in current_jobs.items() for title in titles]
        pd.DataFrame(all_postings).to_csv(self.results_path, index=False, encoding='utf-8-sig')
        self.logger.info(f"ê²°ê³¼ë¥¼ '{self.results_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    def send_slack_notification(self, new_jobs: Dict, warnings: List, failed_companies: List):
        if not self.webhook_url:
            self.logger.error(f"{self.webhook_url_env}ì´ .envì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        if not new_jobs and not warnings and not failed_companies:
            self.logger.info("ì•Œë¦¼ ë³´ë‚¼ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        current_time = datetime.now().strftime('%H:%M')
        messages_to_send = []

        # 1. ìƒˆë¡œìš´ ì±„ìš©ê³µê³  ë©”ì‹œì§€ë“¤ ìƒì„±
        if new_jobs:
            total_new_jobs = sum(len(jobs) for jobs in new_jobs.values())
            header_msg = f"ğŸ‰ *ìƒˆë¡œìš´ ì±„ìš©ê³µê³  {total_new_jobs}ê°œ ë°œê²¬!* ({current_time})\n"

            current_message = header_msg
            for company, jobs in new_jobs.items():
                company_url = self.company_urls.get(company, "")
                linked_company = f"<{company_url}|{company}>" if company_url else company
                company_section = f"\nğŸ“¢ *{linked_company}* - {len(jobs)}ê°œ\n"

                for job in jobs:
                    company_section += f"â€¢ {job}\n"

                # ë©”ì‹œì§€ê°€ 3500ìë¥¼ ì´ˆê³¼í•˜ë©´ ë¶„í• 
                if len(current_message + company_section) > 3500:
                    messages_to_send.append(current_message.strip())
                    current_message = company_section
                else:
                    current_message += company_section

            if current_message.strip():
                messages_to_send.append(current_message.strip())

        # 2. ê²½ê³  ë©”ì‹œì§€
        if warnings:
            warning_msg = "âš ï¸ *ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê²°ê³¼* (ì§ì ‘ í™•ì¸ í•„ìš”)\n"
            for warning in warnings:
                warning_msg += f"â€¢ {warning}\n"
            messages_to_send.append(warning_msg.strip())

        # 3. ì‹¤íŒ¨ ë©”ì‹œì§€
        if failed_companies:
            fail_msg = "âŒ *í¬ë¡¤ë§ ì‹¤íŒ¨*\n"
            for fail in failed_companies:
                fail_line = f"â€¢ {fail['company']}: {fail['reason']}\n"
                # ì‹¤íŒ¨ ë©”ì‹œì§€ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ë¶„í• 
                if len(fail_msg + fail_line) > 3500:
                    messages_to_send.append(fail_msg.strip())
                    fail_msg = f"âŒ *í¬ë¡¤ë§ ì‹¤íŒ¨ (ê³„ì†)*\n{fail_line}"
                else:
                    fail_msg += fail_line

            if fail_msg.strip() != "âŒ *í¬ë¡¤ë§ ì‹¤íŒ¨*":
                messages_to_send.append(fail_msg.strip())

        # ë©”ì‹œì§€ë“¤ ì „ì†¡
        for i, message in enumerate(messages_to_send):
            payload = {"text": message, "username": "ì±„ìš©ê³µê³  ì•Œë¦¬ë¯¸", "icon_emoji": ":robot_face:"}
            try:
                self.logger.info(f"ğŸ“¤ ìŠ¬ë™ ë©”ì‹œì§€ ì „ì†¡ ì‹œë„ ({i+1}/{len(messages_to_send)}) - ë©”ì‹œì§€ ê¸¸ì´: {len(message)}ì")
                response = requests.post(self.webhook_url, json=payload, timeout=15)

                if response.status_code == 200:
                    self.logger.info(f"âœ… ìŠ¬ë™ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ ({i+1}/{len(messages_to_send)})")
                else:
                    self.logger.error(f"âŒ ìŠ¬ë™ ì‘ë‹µ ì˜¤ë¥˜ ({i+1}/{len(messages_to_send)}): {response.status_code} - {response.text}")

                # ìŠ¬ë™ ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                import time
                time.sleep(1)

            except Exception as e:
                self.logger.error(f"âŒ ìŠ¬ë™ ì•Œë¦¼ ì „ì†¡ ì˜¤ë¥˜ ({i+1}/{len(messages_to_send)}): {e}")

def main():
    base_dir = os.path.dirname(os.path.abspath(__file__))
    dag = JobMonitoringDAG(base_dir)
    dag.run()

if __name__ == "__main__":
    main()